//
// Generated by LLVM NVPTX Back-End
//

.version 8.0
.target sm_90a
.address_size 64

	// .globl	_kernel_matmul_fp8_block_fastacc
.extern .shared .align 16 .b8 global_smem[];

.visible .entry _kernel_matmul_fp8_block_fastacc(
	.param .u64 _kernel_matmul_fp8_block_fastacc_param_0,
	.param .u64 _kernel_matmul_fp8_block_fastacc_param_1,
	.param .u64 _kernel_matmul_fp8_block_fastacc_param_2,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_3,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_4,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_5,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_6,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_7,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_8,
	.param .u64 _kernel_matmul_fp8_block_fastacc_param_9,
	.param .u64 _kernel_matmul_fp8_block_fastacc_param_10,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_11,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_12,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_13,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_14,
	.param .u32 _kernel_matmul_fp8_block_fastacc_param_15
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<133>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<803>;
	.reg .f32 	%f<3559>;
	.reg .b64 	%rd<274>;
	.loc	1 1061 0
$L__func_begin0:
	.loc	1 1061 0

	ld.param.u32 	%r55, [_kernel_matmul_fp8_block_fastacc_param_4];
	ld.param.u32 	%r54, [_kernel_matmul_fp8_block_fastacc_param_3];
	ld.param.u64 	%rd42, [_kernel_matmul_fp8_block_fastacc_param_1];
	ld.param.u64 	%rd41, [_kernel_matmul_fp8_block_fastacc_param_0];
$L__tmp0:
	.loc	1 1145 24
	// begin inline asm
	mov.u32 %r57, %ctaid.x;
	// end inline asm
	.loc	1 1146 26
	// begin inline asm
	mov.u32 %r58, %ctaid.y;
	// end inline asm
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r157, %r54, 127;
	.loc	2 44 28
	shr.s32 	%r158, %r157, 31;
	shr.u32 	%r159, %r158, 25;
	add.s32 	%r160, %r157, %r159;
	shr.s32 	%r161, %r160, 7;
$L__tmp2:
	.loc	2 44 22
	add.s32 	%r162, %r55, 127;
	ld.param.u32 	%r163, [_kernel_matmul_fp8_block_fastacc_param_5];
	.loc	2 44 28
	shr.s32 	%r164, %r162, 31;
	shr.u32 	%r165, %r164, 25;
	add.s32 	%r166, %r162, %r165;
	shr.s32 	%r167, %r166, 7;
$L__tmp3:
	.loc	1 1151 22
	shl.b32 	%r169, %r167, 3;
	ld.param.u64 	%rd102, [_kernel_matmul_fp8_block_fastacc_param_9];
	.loc	1 1152 22
	div.s32 	%r170, %r57, %r169;
	ld.param.u64 	%rd103, [_kernel_matmul_fp8_block_fastacc_param_10];
	.loc	1 1153 41
	shl.b32 	%r171, %r170, 3;
	ld.param.u32 	%r172, [_kernel_matmul_fp8_block_fastacc_param_11];
	.loc	1 1153 30
	sub.s32 	%r173, %r161, %r171;
	ld.param.u32 	%r174, [_kernel_matmul_fp8_block_fastacc_param_12];
	.loc	1 1153 50
	min.s32 	%r175, %r173, 8;
	.loc	1 1154 40
	rem.s32 	%r176, %r57, %r175;
	ld.param.u32 	%r177, [_kernel_matmul_fp8_block_fastacc_param_14];
	.loc	1 1154 34
	add.s32 	%r178, %r171, %r176;
	ld.param.u32 	%r179, [_kernel_matmul_fp8_block_fastacc_param_15];
	mul.lo.s32 	%r180, %r170, %r169;
	sub.s32 	%r181, %r57, %r180;
	.loc	1 1155 30
	div.s32 	%r182, %r181, %r175;
	.loc	1 1157 17
	shl.b32 	%r2, %r178, 7;
	.loc	1 1157 40
	mov.u32 	%r3, %tid.x;
	shr.u32 	%r4, %r3, 5;
	bfe.u32 	%r183, %r3, 3, 4;
	or.b32  	%r184, %r183, 16;
	or.b32  	%r185, %r183, 32;
	or.b32  	%r186, %r183, 48;
	or.b32  	%r187, %r183, 64;
	or.b32  	%r188, %r183, 80;
	or.b32  	%r189, %r183, 96;
	or.b32  	%r190, %r183, 112;
	shl.b32 	%r191, %r3, 4;
	and.b32  	%r5, %r191, 112;
	.loc	1 1157 27
	or.b32  	%r192, %r2, %r183;
	or.b32  	%r193, %r2, %r184;
	or.b32  	%r194, %r2, %r185;
	or.b32  	%r195, %r2, %r186;
	or.b32  	%r196, %r2, %r187;
	or.b32  	%r197, %r2, %r188;
	or.b32  	%r198, %r2, %r189;
	or.b32  	%r199, %r2, %r190;
	.loc	1 1158 17
	shl.b32 	%r6, %r182, 7;
	.loc	1 1158 27
	or.b32  	%r200, %r6, %r183;
	or.b32  	%r201, %r6, %r184;
	or.b32  	%r202, %r6, %r185;
	or.b32  	%r203, %r6, %r186;
	or.b32  	%r204, %r6, %r187;
	or.b32  	%r205, %r6, %r188;
	or.b32  	%r206, %r6, %r189;
	or.b32  	%r207, %r6, %r190;
	.loc	1 1159 48
	rem.s32 	%r208, %r192, %r54;
	rem.s32 	%r209, %r193, %r54;
	rem.s32 	%r210, %r194, %r54;
	rem.s32 	%r211, %r195, %r54;
	rem.s32 	%r212, %r196, %r54;
	rem.s32 	%r213, %r197, %r54;
	rem.s32 	%r214, %r198, %r54;
	rem.s32 	%r215, %r199, %r54;
	.loc	1 1160 48
	rem.s32 	%r216, %r200, %r55;
	rem.s32 	%r217, %r201, %r55;
	rem.s32 	%r218, %r202, %r55;
	rem.s32 	%r219, %r203, %r55;
	rem.s32 	%r220, %r204, %r55;
	rem.s32 	%r221, %r205, %r55;
	rem.s32 	%r222, %r206, %r55;
	rem.s32 	%r223, %r207, %r55;
	.loc	1 1161 17
	shl.b32 	%r7, %r58, 7;
	.loc	1 1161 27
	or.b32  	%r224, %r7, %r5;
	.loc	1 1163 28
	mul.lo.s32 	%r8, %r208, %r172;
	mul.lo.s32 	%r9, %r209, %r172;
	mul.lo.s32 	%r10, %r210, %r172;
	mul.lo.s32 	%r11, %r211, %r172;
	mul.lo.s32 	%r12, %r212, %r172;
	mul.lo.s32 	%r13, %r213, %r172;
	mul.lo.s32 	%r14, %r214, %r172;
	mul.lo.s32 	%r15, %r215, %r172;
	.loc	1 1163 40
	add.s32 	%r225, %r8, %r224;
	add.s32 	%r226, %r9, %r224;
	add.s32 	%r227, %r10, %r224;
	add.s32 	%r228, %r11, %r224;
	add.s32 	%r229, %r12, %r224;
	add.s32 	%r230, %r13, %r224;
	add.s32 	%r231, %r14, %r224;
	add.s32 	%r232, %r15, %r224;
	.loc	1 1163 13
	cvt.s64.s32 	%rd104, %r225;
	add.s64 	%rd44, %rd41, %rd104;
	cvt.s64.s32 	%rd105, %r226;
	add.s64 	%rd45, %rd41, %rd105;
	cvt.s64.s32 	%rd106, %r227;
	add.s64 	%rd46, %rd41, %rd106;
	cvt.s64.s32 	%rd107, %r228;
	add.s64 	%rd47, %rd41, %rd107;
	cvt.s64.s32 	%rd108, %r229;
	add.s64 	%rd48, %rd41, %rd108;
	cvt.s64.s32 	%rd109, %r230;
	add.s64 	%rd49, %rd41, %rd109;
	cvt.s64.s32 	%rd110, %r231;
	add.s64 	%rd50, %rd41, %rd110;
	cvt.s64.s32 	%rd111, %r232;
	add.s64 	%rd51, %rd41, %rd111;
	.loc	1 1164 54
	mul.lo.s32 	%r16, %r216, %r174;
	mul.lo.s32 	%r17, %r217, %r174;
	mul.lo.s32 	%r18, %r218, %r174;
	mul.lo.s32 	%r19, %r219, %r174;
	mul.lo.s32 	%r20, %r220, %r174;
	mul.lo.s32 	%r21, %r221, %r174;
	mul.lo.s32 	%r22, %r222, %r174;
	mul.lo.s32 	%r23, %r223, %r174;
	.loc	1 1164 39
	add.s32 	%r233, %r16, %r224;
	add.s32 	%r234, %r17, %r224;
	add.s32 	%r235, %r18, %r224;
	add.s32 	%r236, %r19, %r224;
	add.s32 	%r237, %r20, %r224;
	add.s32 	%r238, %r21, %r224;
	add.s32 	%r239, %r22, %r224;
	add.s32 	%r240, %r23, %r224;
	.loc	1 1164 13
	cvt.s64.s32 	%rd112, %r233;
	add.s64 	%rd52, %rd42, %rd112;
	cvt.s64.s32 	%rd113, %r234;
	add.s64 	%rd53, %rd42, %rd113;
	cvt.s64.s32 	%rd114, %r235;
	add.s64 	%rd54, %rd42, %rd114;
	cvt.s64.s32 	%rd115, %r236;
	add.s64 	%rd55, %rd42, %rd115;
	cvt.s64.s32 	%rd116, %r237;
	add.s64 	%rd56, %rd42, %rd116;
	cvt.s64.s32 	%rd117, %r238;
	add.s64 	%rd57, %rd42, %rd117;
	cvt.s64.s32 	%rd118, %r239;
	add.s64 	%rd58, %rd42, %rd118;
	cvt.s64.s32 	%rd119, %r240;
	add.s64 	%rd59, %rd42, %rd119;
	.loc	1 1167 33
	bfe.s32 	%r241, %r178, 0, 25;
	.loc	1 1168 33
	bfe.s32 	%r242, %r182, 0, 25;
$L__tmp4:
	.loc	2 44 22
	add.s32 	%r24, %r163, 127;
$L__tmp5:
	.loc	1 1205 36
	mul.lo.s32 	%r246, %r241, %r177;
	.loc	1 1205 26
	mul.wide.s32 	%rd120, %r246, 4;
	add.s64 	%rd17, %rd102, %rd120;
	.loc	1 1208 36
	mul.lo.s32 	%r247, %r242, %r179;
	.loc	1 1208 26
	mul.wide.s32 	%rd121, %r247, 4;
	add.s64 	%rd18, %rd103, %rd121;
	.loc	1 1171 22
	setp.gt.s32 	%p51, %r24, 127;
	.loc	1 1174 24
	shl.b32 	%r248, %r183, 7;
	shl.b32 	%r249, %r3, 1;
	xor.b32  	%r250, %r191, %r249;
	and.b32  	%r251, %r250, 112;
	or.b32  	%r26, %r248, %r251;
	mov.u32 	%r252, global_smem;
	add.s32 	%r59, %r252, %r26;
	shl.b32 	%r253, %r184, 7;
	or.b32  	%r27, %r253, %r251;
	add.s32 	%r61, %r252, %r27;
	shl.b32 	%r254, %r185, 7;
	or.b32  	%r28, %r254, %r251;
	add.s32 	%r63, %r252, %r28;
	shl.b32 	%r255, %r186, 7;
	or.b32  	%r29, %r255, %r251;
	add.s32 	%r65, %r252, %r29;
	shl.b32 	%r256, %r187, 7;
	or.b32  	%r30, %r256, %r251;
	add.s32 	%r67, %r252, %r30;
	shl.b32 	%r257, %r188, 7;
	or.b32  	%r31, %r257, %r251;
	add.s32 	%r69, %r252, %r31;
	shl.b32 	%r258, %r189, 7;
	or.b32  	%r32, %r258, %r251;
	add.s32 	%r71, %r252, %r32;
	shl.b32 	%r259, %r190, 7;
	or.b32  	%r33, %r259, %r251;
	add.s32 	%r73, %r252, %r33;
	selp.b32 	%r60, 16, 0, %p51;
	mov.pred 	%p55, -1;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r59 + 0 ], [ %rd44 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r61 + 0 ], [ %rd45 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r63 + 0 ], [ %rd46 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r65 + 0 ], [ %rd47 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r67 + 0 ], [ %rd48 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r69 + 0 ], [ %rd49 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r71 + 0 ], [ %rd50 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r73 + 0 ], [ %rd51 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1175 24
	add.s32 	%r260, %r252, 65536;
	add.s32 	%r75, %r260, %r26;
	add.s32 	%r77, %r260, %r27;
	add.s32 	%r79, %r260, %r28;
	add.s32 	%r81, %r260, %r29;
	add.s32 	%r83, %r260, %r30;
	add.s32 	%r85, %r260, %r31;
	add.s32 	%r87, %r260, %r32;
	add.s32 	%r89, %r260, %r33;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r75 + 0 ], [ %rd52 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r77 + 0 ], [ %rd53 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r79 + 0 ], [ %rd54 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r81 + 0 ], [ %rd55 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r83 + 0 ], [ %rd56 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r85 + 0 ], [ %rd57 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r87 + 0 ], [ %rd58 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r89 + 0 ], [ %rd59 + 0 ], 0x10, %r60;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1171 22
	setp.gt.s32 	%p52, %r24, 255;
	.loc	1 1187 13
	add.s64 	%rd60, %rd44, 128;
	add.s64 	%rd61, %rd45, 128;
	add.s64 	%rd62, %rd46, 128;
	add.s64 	%rd63, %rd47, 128;
	add.s64 	%rd64, %rd48, 128;
	add.s64 	%rd65, %rd49, 128;
	add.s64 	%rd66, %rd50, 128;
	add.s64 	%rd67, %rd51, 128;
	.loc	1 1188 13
	add.s64 	%rd68, %rd52, 128;
	add.s64 	%rd69, %rd53, 128;
	add.s64 	%rd70, %rd54, 128;
	add.s64 	%rd71, %rd55, 128;
	add.s64 	%rd72, %rd56, 128;
	add.s64 	%rd73, %rd57, 128;
	add.s64 	%rd74, %rd58, 128;
	add.s64 	%rd75, %rd59, 128;
	.loc	1 1174 24
	bar.sync 	0;
	add.s32 	%r261, %r252, 16384;
	add.s32 	%r91, %r261, %r26;
	add.s32 	%r93, %r261, %r27;
	add.s32 	%r95, %r261, %r28;
	add.s32 	%r97, %r261, %r29;
	add.s32 	%r99, %r261, %r30;
	add.s32 	%r101, %r261, %r31;
	add.s32 	%r103, %r261, %r32;
	add.s32 	%r105, %r261, %r33;
	selp.b32 	%r92, 16, 0, %p52;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r91 + 0 ], [ %rd60 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r93 + 0 ], [ %rd61 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r95 + 0 ], [ %rd62 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r97 + 0 ], [ %rd63 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r99 + 0 ], [ %rd64 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r101 + 0 ], [ %rd65 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r103 + 0 ], [ %rd66 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r105 + 0 ], [ %rd67 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1175 24
	add.s32 	%r262, %r252, 81920;
	add.s32 	%r107, %r262, %r26;
	add.s32 	%r109, %r262, %r27;
	add.s32 	%r111, %r262, %r28;
	add.s32 	%r113, %r262, %r29;
	add.s32 	%r115, %r262, %r30;
	add.s32 	%r117, %r262, %r31;
	add.s32 	%r119, %r262, %r32;
	add.s32 	%r121, %r262, %r33;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r107 + 0 ], [ %rd68 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r109 + 0 ], [ %rd69 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r111 + 0 ], [ %rd70 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r113 + 0 ], [ %rd71 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r115 + 0 ], [ %rd72 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r117 + 0 ], [ %rd73 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r119 + 0 ], [ %rd74 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r121 + 0 ], [ %rd75 + 0 ], 0x10, %r92;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1171 22
	setp.gt.s32 	%p53, %r24, 383;
	.loc	1 1187 13
	add.s64 	%rd76, %rd44, 256;
	add.s64 	%rd77, %rd45, 256;
	add.s64 	%rd78, %rd46, 256;
	add.s64 	%rd79, %rd47, 256;
	add.s64 	%rd80, %rd48, 256;
	add.s64 	%rd81, %rd49, 256;
	add.s64 	%rd82, %rd50, 256;
	add.s64 	%rd83, %rd51, 256;
	.loc	1 1188 13
	add.s64 	%rd84, %rd52, 256;
	add.s64 	%rd85, %rd53, 256;
	add.s64 	%rd86, %rd54, 256;
	add.s64 	%rd87, %rd55, 256;
	add.s64 	%rd88, %rd56, 256;
	add.s64 	%rd89, %rd57, 256;
	add.s64 	%rd90, %rd58, 256;
	add.s64 	%rd91, %rd59, 256;
	.loc	1 1174 24
	bar.sync 	0;
	add.s32 	%r263, %r252, 32768;
	add.s32 	%r123, %r263, %r26;
	add.s32 	%r125, %r263, %r27;
	add.s32 	%r127, %r263, %r28;
	add.s32 	%r129, %r263, %r29;
	add.s32 	%r131, %r263, %r30;
	add.s32 	%r133, %r263, %r31;
	add.s32 	%r135, %r263, %r32;
	add.s32 	%r137, %r263, %r33;
	selp.b32 	%r124, 16, 0, %p53;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r123 + 0 ], [ %rd76 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r125 + 0 ], [ %rd77 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r127 + 0 ], [ %rd78 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r129 + 0 ], [ %rd79 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r131 + 0 ], [ %rd80 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r133 + 0 ], [ %rd81 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r135 + 0 ], [ %rd82 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r137 + 0 ], [ %rd83 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1175 24
	add.s32 	%r264, %r252, 98304;
	add.s32 	%r139, %r264, %r26;
	add.s32 	%r141, %r264, %r27;
	add.s32 	%r143, %r264, %r28;
	add.s32 	%r145, %r264, %r29;
	add.s32 	%r147, %r264, %r30;
	add.s32 	%r149, %r264, %r31;
	add.s32 	%r151, %r264, %r32;
	add.s32 	%r153, %r264, %r33;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r139 + 0 ], [ %rd84 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r141 + 0 ], [ %rd85 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r143 + 0 ], [ %rd86 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r145 + 0 ], [ %rd87 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r147 + 0 ], [ %rd88 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r149 + 0 ], [ %rd89 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r151 + 0 ], [ %rd90 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r153 + 0 ], [ %rd91 + 0 ], 0x10, %r124;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1174 24
	// begin inline asm
	cp.async.wait_group 0x4;
	// end inline asm
	bar.sync 	0;
	.loc	1 1185 27
	and.b32  	%r35, %r4, 134217724;
	shfl.sync.idx.b32	%r265, %r35, 0, 31, -1;
	// begin inline asm
	wgmma.fence.sync.aligned;
	// end inline asm
	shl.b32 	%r266, %r265, 7;
	and.b32  	%r267, %r266, 384;
	cvt.u64.u32 	%rd122, %r267;
	shr.u32 	%r268, %r252, 4;
	cvt.u64.u32 	%rd123, %r268;
	and.b64  	%rd124, %rd123, 16383;
	add.s64 	%rd125, %rd124, %rd122;
	or.b64  	%rd92, %rd125, 4611686293338849280;
	shr.u32 	%r269, %r260, 4;
	cvt.u64.u32 	%rd126, %r269;
	and.b64  	%rd127, %rd126, 16383;
	or.b64  	%rd133, %rd127, 4611686293372403712;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f905,%f906,%f907,%f908,%f909,%f910,%f911,%f912,%f913,%f914,%f915,%f916,%f917,%f918,%f919,%f920,%f921,%f922,%f923,%f924,%f925,%f926,%f927,%f928,%f929,%f930,%f931,%f932,%f933,%f934,%f935,%f936,%f937,%f938,%f939,%f940,%f941,%f942,%f943,%f944,%f945,%f946,%f947,%f948,%f949,%f950,%f951,%f952,%f953,%f954,%f955,%f956,%f957,%f958,%f959,%f960,%f961,%f962,%f963,%f964,%f965,%f966,%f967,%f968}, %rd92, %rd133, 0, 1, 1;
	// end inline asm
	add.s64 	%rd94, %rd125, 4611686293338849282;
	add.s64 	%rd135, %rd127, 4611686293372403714;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f905,%f906,%f907,%f908,%f909,%f910,%f911,%f912,%f913,%f914,%f915,%f916,%f917,%f918,%f919,%f920,%f921,%f922,%f923,%f924,%f925,%f926,%f927,%f928,%f929,%f930,%f931,%f932,%f933,%f934,%f935,%f936,%f937,%f938,%f939,%f940,%f941,%f942,%f943,%f944,%f945,%f946,%f947,%f948,%f949,%f950,%f951,%f952,%f953,%f954,%f955,%f956,%f957,%f958,%f959,%f960,%f961,%f962,%f963,%f964,%f965,%f966,%f967,%f968}, %rd94, %rd135, 1, 1, 1;
	// end inline asm
	add.s64 	%rd96, %rd125, 4611686293338849284;
	add.s64 	%rd137, %rd127, 4611686293372403716;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f905,%f906,%f907,%f908,%f909,%f910,%f911,%f912,%f913,%f914,%f915,%f916,%f917,%f918,%f919,%f920,%f921,%f922,%f923,%f924,%f925,%f926,%f927,%f928,%f929,%f930,%f931,%f932,%f933,%f934,%f935,%f936,%f937,%f938,%f939,%f940,%f941,%f942,%f943,%f944,%f945,%f946,%f947,%f948,%f949,%f950,%f951,%f952,%f953,%f954,%f955,%f956,%f957,%f958,%f959,%f960,%f961,%f962,%f963,%f964,%f965,%f966,%f967,%f968}, %rd96, %rd137, 1, 1, 1;
	// end inline asm
	add.s64 	%rd98, %rd125, 4611686293338849286;
	add.s64 	%rd139, %rd127, 4611686293372403718;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f905,%f906,%f907,%f908,%f909,%f910,%f911,%f912,%f913,%f914,%f915,%f916,%f917,%f918,%f919,%f920,%f921,%f922,%f923,%f924,%f925,%f926,%f927,%f928,%f929,%f930,%f931,%f932,%f933,%f934,%f935,%f936,%f937,%f938,%f939,%f940,%f941,%f942,%f943,%f944,%f945,%f946,%f947,%f948,%f949,%f950,%f951,%f952,%f953,%f954,%f955,%f956,%f957,%f958,%f959,%f960,%f961,%f962,%f963,%f964,%f965,%f966,%f967,%f968}, %rd98, %rd139, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned;
	// end inline asm
	mov.b32 	%f1225, %r252;
	mov.b32 	%f1831, %r260;
	mov.f32 	%f1301, 0f00000001;
	mov.f32 	%f1300, 0f00000080;
	mov.f32 	%f1302, 0f00000000;
	mov.f32 	%f1231, %f1301;
	mov.f32 	%f1228, %f1302;
	mov.f32 	%f1229, %f1302;
	mov.f32 	%f1233, %f1302;
	mov.f32 	%f1232, %f1300;
	mov.f32 	%f1226, %f1300;
	mov.f32 	%f1230, %f1831;
	mov.f32 	%f1234, %f1302;
	mov.f32 	%f1227, %f1301;
	// begin inline asm
	// wait for regs: %f905,%f906,%f907,%f908,%f909,%f910,%f911,%f912,%f913,%f914,%f915,%f916,%f917,%f918,%f919,%f920,%f921,%f922,%f923,%f924,%f925,%f926,%f927,%f928,%f929,%f930,%f931,%f932,%f933,%f934,%f935,%f936,%f937,%f938,%f939,%f940,%f941,%f942,%f943,%f944,%f945,%f946,%f947,%f948,%f949,%f950,%f951,%f952,%f953,%f954,%f955,%f956,%f957,%f958,%f959,%f960,%f961,%f962,%f963,%f964,%f965,%f966,%f967,%f968,%f1225,%f1226,%f1227,%f1228,%f1229,%f1230,%f1231,%f1232,%f1233,%f1234
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	.loc	1 1205 54
	mul.wide.s32 	%rd128, %r58, 4;
	add.s64 	%rd100, %rd17, %rd128;
	.loc	1 1205 16
	// begin inline asm
	mov.u32 %r155, 0x0;
	@%p55 ld.global.b32 { %r155 }, [ %rd100 + 0 ];
	// end inline asm
	mov.b32 	%f1309, %r155;
	.loc	1 1208 54
	add.s64 	%rd101, %rd18, %rd128;
	.loc	1 1208 16
	// begin inline asm
	mov.u32 %r156, 0x0;
	@%p55 ld.global.b32 { %r156 }, [ %rd101 + 0 ];
	// end inline asm
	mov.b32 	%f1310, %r156;
	.loc	1 1210 30
	mul.f32 	%f3494, %f1309, %f1310;
	.loc	1 1211 24
	add.s32 	%r270, %r163, -1;
	setp.lt.u32 	%p54, %r270, 128;
	.loc	1 1211 15
	@%p54 bra 	$L__BB0_2;
	.loc	1 1203 37
	add.s32 	%r276, %r58, 1;
	.loc	1 1215 58
	mul.wide.s32 	%rd131, %r276, 4;
	add.s64 	%rd129, %rd17, %rd131;
	.loc	1 1215 20
	// begin inline asm
	mov.u32 %r271, 0x0;
	@%p55 ld.global.b32 { %r271 }, [ %rd129 + 0 ];
	// end inline asm
	mov.b32 	%f1311, %r271;
	.loc	1 1218 58
	add.s64 	%rd130, %rd18, %rd131;
	.loc	1 1218 20
	// begin inline asm
	mov.u32 %r272, 0x0;
	@%p55 ld.global.b32 { %r272 }, [ %rd130 + 0 ];
	// end inline asm
	mov.b32 	%f1312, %r272;
	.loc	1 1220 44
	mul.f32 	%f1313, %f1311, %f1312;
	.loc	1 1221 47
	mov.b32 	%r275, %f1313;
	mov.b32 	%r274, %f3494;
	// begin inline asm
	div.full.f32 %r273, %r274, %r275;
	// end inline asm
	mov.b32 	%f3494, %r273;
$L__BB0_2:
	.loc	1 0 47
	ld.param.u32 	%r56, [_kernel_matmul_fp8_block_fastacc_param_13];
	ld.param.u64 	%rd43, [_kernel_matmul_fp8_block_fastacc_param_2];
	.loc	1 1171 22
	setp.gt.s32 	%p73, %r24, 511;
	setp.lt.s32 	%p74, %r24, 256;
	.loc	1 1222 19
	mul.f32 	%f3428, %f905, %f3494;
	mul.f32 	%f3427, %f906, %f3494;
	mul.f32 	%f3426, %f907, %f3494;
	mul.f32 	%f3425, %f908, %f3494;
	mul.f32 	%f3424, %f909, %f3494;
	mul.f32 	%f3423, %f910, %f3494;
	mul.f32 	%f3422, %f911, %f3494;
	mul.f32 	%f3421, %f912, %f3494;
	mul.f32 	%f3420, %f913, %f3494;
	mul.f32 	%f3419, %f914, %f3494;
	mul.f32 	%f3418, %f915, %f3494;
	mul.f32 	%f3417, %f916, %f3494;
	mul.f32 	%f3416, %f917, %f3494;
	mul.f32 	%f3415, %f918, %f3494;
	mul.f32 	%f3414, %f919, %f3494;
	mul.f32 	%f3413, %f920, %f3494;
	mul.f32 	%f3412, %f921, %f3494;
	mul.f32 	%f3411, %f922, %f3494;
	mul.f32 	%f3410, %f923, %f3494;
	mul.f32 	%f3409, %f924, %f3494;
	mul.f32 	%f3408, %f925, %f3494;
	mul.f32 	%f3407, %f926, %f3494;
	mul.f32 	%f3406, %f927, %f3494;
	mul.f32 	%f3405, %f928, %f3494;
	mul.f32 	%f3404, %f929, %f3494;
	mul.f32 	%f3403, %f930, %f3494;
	mul.f32 	%f3402, %f931, %f3494;
	mul.f32 	%f3401, %f932, %f3494;
	mul.f32 	%f3400, %f933, %f3494;
	mul.f32 	%f3399, %f934, %f3494;
	mul.f32 	%f3398, %f935, %f3494;
	mul.f32 	%f3397, %f936, %f3494;
	mul.f32 	%f3396, %f937, %f3494;
	mul.f32 	%f3395, %f938, %f3494;
	mul.f32 	%f3394, %f939, %f3494;
	mul.f32 	%f3393, %f940, %f3494;
	mul.f32 	%f3392, %f941, %f3494;
	mul.f32 	%f3391, %f942, %f3494;
	mul.f32 	%f3390, %f943, %f3494;
	mul.f32 	%f3389, %f944, %f3494;
	mul.f32 	%f3388, %f945, %f3494;
	mul.f32 	%f3387, %f946, %f3494;
	mul.f32 	%f3386, %f947, %f3494;
	mul.f32 	%f3385, %f948, %f3494;
	mul.f32 	%f3384, %f949, %f3494;
	mul.f32 	%f3383, %f950, %f3494;
	mul.f32 	%f3382, %f951, %f3494;
	mul.f32 	%f3381, %f952, %f3494;
	mul.f32 	%f3380, %f953, %f3494;
	mul.f32 	%f3379, %f954, %f3494;
	mul.f32 	%f3378, %f955, %f3494;
	mul.f32 	%f3377, %f956, %f3494;
	mul.f32 	%f3376, %f957, %f3494;
	mul.f32 	%f3375, %f958, %f3494;
	mul.f32 	%f3374, %f959, %f3494;
	mul.f32 	%f3373, %f960, %f3494;
	mul.f32 	%f3372, %f961, %f3494;
	mul.f32 	%f3371, %f962, %f3494;
	mul.f32 	%f3370, %f963, %f3494;
	mul.f32 	%f3369, %f964, %f3494;
	mul.f32 	%f3368, %f965, %f3494;
	mul.f32 	%f3367, %f966, %f3494;
	mul.f32 	%f3366, %f967, %f3494;
	mul.f32 	%f3365, %f968, %f3494;
	.loc	1 1185 27
	shfl.sync.idx.b32	%r309, %r35, 0, 31, -1;
	// begin inline asm
	wgmma.fence.sync.aligned;
	// end inline asm
	shl.b32 	%r310, %r309, 7;
	and.b32  	%r311, %r310, 384;
	cvt.u64.u32 	%rd156, %r311;
	add.s32 	%r313, %r252, 8192;
	shr.u32 	%r314, %r313, 4;
	cvt.u64.u32 	%rd157, %r314;
	and.b64  	%rd158, %rd157, 16383;
	add.s64 	%rd159, %rd158, %rd156;
	or.b64  	%rd132, %rd159, 4611686293338849280;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761}, %rd132, %rd133, 0, 1, 1;
	// end inline asm
	add.s64 	%rd134, %rd159, 4611686293338849282;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761}, %rd134, %rd135, 1, 1, 1;
	// end inline asm
	add.s64 	%rd136, %rd159, 4611686293338849284;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761}, %rd136, %rd137, 1, 1, 1;
	// end inline asm
	add.s64 	%rd138, %rd159, 4611686293338849286;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761}, %rd138, %rd139, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned;
	// end inline asm
	mov.b32 	%f1826, %r313;
	mov.f32 	%f1903, 0f00000040;
	mov.f32 	%f1829, %f1903;
	mov.f32 	%f1832, %f1301;
	mov.f32 	%f1830, %f1302;
	mov.f32 	%f1834, %f1302;
	mov.f32 	%f1833, %f1300;
	mov.f32 	%f1827, %f1300;
	mov.f32 	%f1828, %f1301;
	mov.f32 	%f1835, %f1302;
	// begin inline asm
	// wait for regs: %f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761,%f1826,%f1827,%f1828,%f1829,%f1830,%f1831,%f1832,%f1833,%f1834,%f1835
	wgmma.wait_group.sync.aligned 1;
	// end inline asm
	.loc	1 1187 13
	add.s64 	%rd140, %rd44, 384;
	add.s64 	%rd141, %rd45, 384;
	add.s64 	%rd142, %rd46, 384;
	add.s64 	%rd143, %rd47, 384;
	add.s64 	%rd144, %rd48, 384;
	add.s64 	%rd145, %rd49, 384;
	add.s64 	%rd146, %rd50, 384;
	add.s64 	%rd147, %rd51, 384;
	.loc	1 1188 13
	add.s64 	%rd148, %rd52, 384;
	add.s64 	%rd149, %rd53, 384;
	add.s64 	%rd150, %rd54, 384;
	add.s64 	%rd151, %rd55, 384;
	add.s64 	%rd152, %rd56, 384;
	add.s64 	%rd153, %rd57, 384;
	add.s64 	%rd154, %rd58, 384;
	add.s64 	%rd155, %rd59, 384;
	.loc	1 1174 24
	bar.sync 	0;
	add.s32 	%r316, %r252, 49152;
	add.s32 	%r277, %r316, %r26;
	add.s32 	%r279, %r316, %r27;
	add.s32 	%r281, %r316, %r28;
	add.s32 	%r283, %r316, %r29;
	add.s32 	%r285, %r316, %r30;
	add.s32 	%r287, %r316, %r31;
	add.s32 	%r289, %r316, %r32;
	add.s32 	%r291, %r316, %r33;
	selp.b32 	%r278, 16, 0, %p73;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r277 + 0 ], [ %rd140 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r279 + 0 ], [ %rd141 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r281 + 0 ], [ %rd142 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r283 + 0 ], [ %rd143 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r285 + 0 ], [ %rd144 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r287 + 0 ], [ %rd145 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r289 + 0 ], [ %rd146 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r291 + 0 ], [ %rd147 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1175 24
	add.s32 	%r317, %r252, 114688;
	add.s32 	%r293, %r317, %r26;
	add.s32 	%r295, %r317, %r27;
	add.s32 	%r297, %r317, %r28;
	add.s32 	%r299, %r317, %r29;
	add.s32 	%r301, %r317, %r30;
	add.s32 	%r303, %r317, %r31;
	add.s32 	%r305, %r317, %r32;
	add.s32 	%r307, %r317, %r33;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r293 + 0 ], [ %rd148 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r295 + 0 ], [ %rd149 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r297 + 0 ], [ %rd150 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r299 + 0 ], [ %rd151 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r301 + 0 ], [ %rd152 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r303 + 0 ], [ %rd153 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r305 + 0 ], [ %rd154 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r307 + 0 ], [ %rd155 + 0 ], 0x10, %r278;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1171 22
	@%p74 bra 	$L__BB0_7;
	.loc	1 0 22
	shr.s32 	%r243, %r24, 31;
	shr.u32 	%r244, %r243, 25;
	add.s32 	%r245, %r24, %r244;
	shr.s32 	%r25, %r245, 7;
	add.s32 	%r34, %r25, -3;
	.loc	1 1211 15
	add.s32 	%r321, %r23, %r7;
	add.s32 	%r322, %r321, %r5;
	cvt.s64.s32 	%rd161, %r322;
	add.s64 	%rd162, %rd161, %rd42;
	add.s64 	%rd23, %rd162, 512;
	add.s32 	%r323, %r22, %r7;
	add.s32 	%r324, %r323, %r5;
	cvt.s64.s32 	%rd163, %r324;
	add.s64 	%rd164, %rd163, %rd42;
	add.s64 	%rd24, %rd164, 512;
	add.s32 	%r325, %r21, %r7;
	add.s32 	%r326, %r325, %r5;
	cvt.s64.s32 	%rd165, %r326;
	add.s64 	%rd166, %rd165, %rd42;
	add.s64 	%rd25, %rd166, 512;
	add.s32 	%r327, %r20, %r7;
	add.s32 	%r328, %r327, %r5;
	cvt.s64.s32 	%rd167, %r328;
	add.s64 	%rd168, %rd167, %rd42;
	add.s64 	%rd26, %rd168, 512;
	add.s32 	%r329, %r19, %r7;
	add.s32 	%r330, %r329, %r5;
	cvt.s64.s32 	%rd169, %r330;
	add.s64 	%rd170, %rd169, %rd42;
	add.s64 	%rd27, %rd170, 512;
	add.s32 	%r331, %r18, %r7;
	add.s32 	%r332, %r331, %r5;
	cvt.s64.s32 	%rd171, %r332;
	add.s64 	%rd172, %rd171, %rd42;
	add.s64 	%rd28, %rd172, 512;
	add.s32 	%r333, %r17, %r7;
	add.s32 	%r334, %r333, %r5;
	cvt.s64.s32 	%rd173, %r334;
	add.s64 	%rd174, %rd173, %rd42;
	add.s64 	%rd29, %rd174, 512;
	add.s32 	%r335, %r16, %r7;
	add.s32 	%r336, %r335, %r5;
	cvt.s64.s32 	%rd175, %r336;
	add.s64 	%rd176, %rd175, %rd42;
	add.s64 	%rd30, %rd176, 512;
	add.s32 	%r337, %r15, %r7;
	add.s32 	%r338, %r337, %r5;
	cvt.s64.s32 	%rd177, %r338;
	add.s64 	%rd178, %rd177, %rd41;
	add.s64 	%rd31, %rd178, 512;
	add.s32 	%r339, %r14, %r7;
	add.s32 	%r340, %r339, %r5;
	cvt.s64.s32 	%rd179, %r340;
	add.s64 	%rd180, %rd179, %rd41;
	add.s64 	%rd32, %rd180, 512;
	add.s32 	%r341, %r13, %r7;
	add.s32 	%r342, %r341, %r5;
	cvt.s64.s32 	%rd181, %r342;
	add.s64 	%rd182, %rd181, %rd41;
	add.s64 	%rd33, %rd182, 512;
	add.s32 	%r343, %r12, %r7;
	add.s32 	%r344, %r343, %r5;
	cvt.s64.s32 	%rd183, %r344;
	add.s64 	%rd184, %rd183, %rd41;
	add.s64 	%rd34, %rd184, 512;
	add.s32 	%r345, %r11, %r7;
	add.s32 	%r346, %r345, %r5;
	cvt.s64.s32 	%rd185, %r346;
	add.s64 	%rd186, %rd185, %rd41;
	add.s64 	%rd35, %rd186, 512;
	add.s32 	%r347, %r10, %r7;
	add.s32 	%r348, %r347, %r5;
	cvt.s64.s32 	%rd187, %r348;
	add.s64 	%rd188, %rd187, %rd41;
	add.s64 	%rd36, %rd188, 512;
	add.s32 	%r349, %r9, %r7;
	add.s32 	%r350, %r349, %r5;
	cvt.s64.s32 	%rd189, %r350;
	add.s64 	%rd190, %rd189, %rd41;
	add.s64 	%rd37, %rd190, 512;
	add.s32 	%r351, %r8, %r7;
	add.s32 	%r352, %r351, %r5;
	cvt.s64.s32 	%rd191, %r352;
	add.s64 	%rd192, %rd191, %rd41;
	add.s64 	%rd38, %rd192, 512;
	mov.b32 	%r802, 3;
	mov.b32 	%r801, 0;
	mov.u64 	%rd273, 0;
	mov.b32 	%r800, 2;
	mov.f32 	%f3364, %f3494;
	bra.uni 	$L__BB0_4;
$L__BB0_6:
	.loc	1 1171 22
	add.s32 	%r39, %r800, -1;
	setp.lt.s32 	%p97, %r39, %r34;
	.loc	1 1222 19
	mul.f32 	%f3428, %f3428, %f3494;
	mul.f32 	%f3427, %f3427, %f3494;
	mul.f32 	%f3426, %f3426, %f3494;
	mul.f32 	%f3425, %f3425, %f3494;
	mul.f32 	%f3424, %f3424, %f3494;
	mul.f32 	%f3423, %f3423, %f3494;
	mul.f32 	%f3422, %f3422, %f3494;
	mul.f32 	%f3421, %f3421, %f3494;
	mul.f32 	%f3420, %f3420, %f3494;
	mul.f32 	%f3419, %f3419, %f3494;
	mul.f32 	%f3418, %f3418, %f3494;
	mul.f32 	%f3417, %f3417, %f3494;
	mul.f32 	%f3416, %f3416, %f3494;
	mul.f32 	%f3415, %f3415, %f3494;
	mul.f32 	%f3414, %f3414, %f3494;
	mul.f32 	%f3413, %f3413, %f3494;
	mul.f32 	%f3412, %f3412, %f3494;
	mul.f32 	%f3411, %f3411, %f3494;
	mul.f32 	%f3410, %f3410, %f3494;
	mul.f32 	%f3409, %f3409, %f3494;
	mul.f32 	%f3408, %f3408, %f3494;
	mul.f32 	%f3407, %f3407, %f3494;
	mul.f32 	%f3406, %f3406, %f3494;
	mul.f32 	%f3405, %f3405, %f3494;
	mul.f32 	%f3404, %f3404, %f3494;
	mul.f32 	%f3403, %f3403, %f3494;
	mul.f32 	%f3402, %f3402, %f3494;
	mul.f32 	%f3401, %f3401, %f3494;
	mul.f32 	%f3400, %f3400, %f3494;
	mul.f32 	%f3399, %f3399, %f3494;
	mul.f32 	%f3398, %f3398, %f3494;
	mul.f32 	%f3397, %f3397, %f3494;
	mul.f32 	%f3396, %f3396, %f3494;
	mul.f32 	%f3395, %f3395, %f3494;
	mul.f32 	%f3394, %f3394, %f3494;
	mul.f32 	%f3393, %f3393, %f3494;
	mul.f32 	%f3392, %f3392, %f3494;
	mul.f32 	%f3391, %f3391, %f3494;
	mul.f32 	%f3390, %f3390, %f3494;
	mul.f32 	%f3389, %f3389, %f3494;
	mul.f32 	%f3388, %f3388, %f3494;
	mul.f32 	%f3387, %f3387, %f3494;
	mul.f32 	%f3386, %f3386, %f3494;
	mul.f32 	%f3385, %f3385, %f3494;
	mul.f32 	%f3384, %f3384, %f3494;
	mul.f32 	%f3383, %f3383, %f3494;
	mul.f32 	%f3382, %f3382, %f3494;
	mul.f32 	%f3381, %f3381, %f3494;
	mul.f32 	%f3380, %f3380, %f3494;
	mul.f32 	%f3379, %f3379, %f3494;
	mul.f32 	%f3378, %f3378, %f3494;
	mul.f32 	%f3377, %f3377, %f3494;
	mul.f32 	%f3376, %f3376, %f3494;
	mul.f32 	%f3375, %f3375, %f3494;
	mul.f32 	%f3374, %f3374, %f3494;
	mul.f32 	%f3373, %f3373, %f3494;
	mul.f32 	%f3372, %f3372, %f3494;
	mul.f32 	%f3371, %f3371, %f3494;
	mul.f32 	%f3370, %f3370, %f3494;
	mul.f32 	%f3369, %f3369, %f3494;
	mul.f32 	%f3368, %f3368, %f3494;
	mul.f32 	%f3367, %f3367, %f3494;
	mul.f32 	%f3366, %f3366, %f3494;
	mul.f32 	%f3365, %f3365, %f3494;
	.loc	1 1187 13
	add.s64 	%rd225, %rd38, %rd273;
	add.s64 	%rd226, %rd37, %rd273;
	add.s64 	%rd227, %rd36, %rd273;
	add.s64 	%rd228, %rd35, %rd273;
	add.s64 	%rd229, %rd34, %rd273;
	add.s64 	%rd230, %rd33, %rd273;
	add.s64 	%rd231, %rd32, %rd273;
	.loc	1 1188 13
	add.s64 	%rd232, %rd31, %rd273;
	add.s64 	%rd233, %rd30, %rd273;
	add.s64 	%rd234, %rd29, %rd273;
	add.s64 	%rd235, %rd28, %rd273;
	add.s64 	%rd236, %rd27, %rd273;
	add.s64 	%rd237, %rd26, %rd273;
	add.s64 	%rd238, %rd25, %rd273;
	add.s64 	%rd239, %rd24, %rd273;
	.loc	1 1171 22
	add.s64 	%rd240, %rd23, %rd273;
	add.s32 	%r408, %r802, 1;
	setp.lt.s32 	%p98, %r408, 4;
	selp.b32 	%r802, %r408, 0, %p98;
	.loc	1 1174 24
	shl.b32 	%r409, %r802, 14;
	add.s32 	%r411, %r252, %r409;
	bar.sync 	0;
	add.s32 	%r376, %r411, %r26;
	add.s32 	%r378, %r411, %r27;
	add.s32 	%r380, %r411, %r28;
	add.s32 	%r382, %r411, %r29;
	add.s32 	%r384, %r411, %r30;
	add.s32 	%r386, %r411, %r31;
	add.s32 	%r388, %r411, %r32;
	add.s32 	%r390, %r411, %r33;
	selp.b32 	%r377, 16, 0, %p97;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r376 + 0 ], [ %rd225 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r378 + 0 ], [ %rd226 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r380 + 0 ], [ %rd227 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r382 + 0 ], [ %rd228 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r384 + 0 ], [ %rd229 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r386 + 0 ], [ %rd230 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r388 + 0 ], [ %rd231 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r390 + 0 ], [ %rd232 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1175 24
	add.s32 	%r412, %r411, 65536;
	add.s32 	%r392, %r412, %r26;
	add.s32 	%r394, %r412, %r27;
	add.s32 	%r396, %r412, %r28;
	add.s32 	%r398, %r412, %r29;
	add.s32 	%r400, %r412, %r30;
	add.s32 	%r402, %r412, %r31;
	add.s32 	%r404, %r412, %r32;
	add.s32 	%r406, %r412, %r33;
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r392 + 0 ], [ %rd233 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r394 + 0 ], [ %rd234 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r396 + 0 ], [ %rd235 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r398 + 0 ], [ %rd236 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r400 + 0 ], [ %rd237 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r402 + 0 ], [ %rd238 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r404 + 0 ], [ %rd239 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	@%p55 cp.async.cg.shared.global [ %r406 + 0 ], [ %rd240 + 0 ], 0x10, %r377;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 1171 22
	add.s64 	%rd273, %rd273, 128;
	add.s32 	%r53, %r800, 1;
	setp.lt.s32 	%p99, %r800, %r25;
	mov.u32 	%r800, %r53;
	mov.f32 	%f3364, %f3494;
	@%p99 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_7;
$L__BB0_4:
	add.s32 	%r355, %r801, 1;
	setp.lt.s32 	%p77, %r355, 4;
	selp.b32 	%r801, %r355, 0, %p77;
	.loc	1 1174 24
	shl.b32 	%r356, %r801, 14;
	add.s32 	%r358, %r252, %r356;
	add.s32 	%r359, %r358, 8192;
	// begin inline asm
	cp.async.wait_group 0x4;
	// end inline asm
	bar.sync 	0;
	.loc	1 1175 24
	add.s32 	%r360, %r358, 65536;
	.loc	1 1185 27
	shfl.sync.idx.b32	%r361, %r35, 0, 31, -1;
	// begin inline asm
	wgmma.fence.sync.aligned;
	// end inline asm
	shl.b32 	%r362, %r361, 7;
	and.b32  	%r363, %r362, 384;
	cvt.u64.u32 	%rd211, %r363;
	shr.u32 	%r364, %r358, 4;
	cvt.u64.u32 	%rd212, %r364;
	and.b64  	%rd213, %rd212, 16383;
	add.s64 	%rd214, %rd213, %rd211;
	or.b64  	%rd193, %rd214, 4611686293338849280;
	shr.u32 	%r365, %r360, 4;
	cvt.u64.u32 	%rd215, %r365;
	and.b64  	%rd216, %rd215, 16383;
	or.b64  	%rd194, %rd216, 4611686293372403712;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f3428,%f3427,%f3426,%f3425,%f3424,%f3423,%f3422,%f3421,%f3420,%f3419,%f3418,%f3417,%f3416,%f3415,%f3414,%f3413,%f3412,%f3411,%f3410,%f3409,%f3408,%f3407,%f3406,%f3405,%f3404,%f3403,%f3402,%f3401,%f3400,%f3399,%f3398,%f3397,%f3396,%f3395,%f3394,%f3393,%f3392,%f3391,%f3390,%f3389,%f3388,%f3387,%f3386,%f3385,%f3384,%f3383,%f3382,%f3381,%f3380,%f3379,%f3378,%f3377,%f3376,%f3375,%f3374,%f3373,%f3372,%f3371,%f3370,%f3369,%f3368,%f3367,%f3366,%f3365}, %rd193, %rd194, 1, 1, 1;
	// end inline asm
	add.s64 	%rd195, %rd214, 4611686293338849282;
	add.s64 	%rd196, %rd216, 4611686293372403714;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f3428,%f3427,%f3426,%f3425,%f3424,%f3423,%f3422,%f3421,%f3420,%f3419,%f3418,%f3417,%f3416,%f3415,%f3414,%f3413,%f3412,%f3411,%f3410,%f3409,%f3408,%f3407,%f3406,%f3405,%f3404,%f3403,%f3402,%f3401,%f3400,%f3399,%f3398,%f3397,%f3396,%f3395,%f3394,%f3393,%f3392,%f3391,%f3390,%f3389,%f3388,%f3387,%f3386,%f3385,%f3384,%f3383,%f3382,%f3381,%f3380,%f3379,%f3378,%f3377,%f3376,%f3375,%f3374,%f3373,%f3372,%f3371,%f3370,%f3369,%f3368,%f3367,%f3366,%f3365}, %rd195, %rd196, 1, 1, 1;
	// end inline asm
	add.s64 	%rd197, %rd214, 4611686293338849284;
	add.s64 	%rd198, %rd216, 4611686293372403716;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f3428,%f3427,%f3426,%f3425,%f3424,%f3423,%f3422,%f3421,%f3420,%f3419,%f3418,%f3417,%f3416,%f3415,%f3414,%f3413,%f3412,%f3411,%f3410,%f3409,%f3408,%f3407,%f3406,%f3405,%f3404,%f3403,%f3402,%f3401,%f3400,%f3399,%f3398,%f3397,%f3396,%f3395,%f3394,%f3393,%f3392,%f3391,%f3390,%f3389,%f3388,%f3387,%f3386,%f3385,%f3384,%f3383,%f3382,%f3381,%f3380,%f3379,%f3378,%f3377,%f3376,%f3375,%f3374,%f3373,%f3372,%f3371,%f3370,%f3369,%f3368,%f3367,%f3366,%f3365}, %rd197, %rd198, 1, 1, 1;
	// end inline asm
	add.s64 	%rd199, %rd214, 4611686293338849286;
	add.s64 	%rd200, %rd216, 4611686293372403718;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f3428,%f3427,%f3426,%f3425,%f3424,%f3423,%f3422,%f3421,%f3420,%f3419,%f3418,%f3417,%f3416,%f3415,%f3414,%f3413,%f3412,%f3411,%f3410,%f3409,%f3408,%f3407,%f3406,%f3405,%f3404,%f3403,%f3402,%f3401,%f3400,%f3399,%f3398,%f3397,%f3396,%f3395,%f3394,%f3393,%f3392,%f3391,%f3390,%f3389,%f3388,%f3387,%f3386,%f3385,%f3384,%f3383,%f3382,%f3381,%f3380,%f3379,%f3378,%f3377,%f3376,%f3375,%f3374,%f3373,%f3372,%f3371,%f3370,%f3369,%f3368,%f3367,%f3366,%f3365}, %rd199, %rd200, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned;
	// end inline asm
	mov.b32 	%f2486, %r358;
	mov.b32 	%f3151, %r360;
	mov.f32 	%f2487, %f1300;
	mov.f32 	%f2493, %f1300;
	mov.f32 	%f2491, %f3151;
	mov.f32 	%f2488, %f1301;
	mov.f32 	%f2492, %f1301;
	mov.f32 	%f2489, %f1302;
	mov.f32 	%f2490, %f1302;
	mov.f32 	%f2494, %f1302;
	mov.f32 	%f2495, %f1302;
	// begin inline asm
	// wait for regs: %f3428,%f3427,%f3426,%f3425,%f3424,%f3423,%f3422,%f3421,%f3420,%f3419,%f3418,%f3417,%f3416,%f3415,%f3414,%f3413,%f3412,%f3411,%f3410,%f3409,%f3408,%f3407,%f3406,%f3405,%f3404,%f3403,%f3402,%f3401,%f3400,%f3399,%f3398,%f3397,%f3396,%f3395,%f3394,%f3393,%f3392,%f3391,%f3390,%f3389,%f3388,%f3387,%f3386,%f3385,%f3384,%f3383,%f3382,%f3381,%f3380,%f3379,%f3378,%f3377,%f3376,%f3375,%f3374,%f3373,%f3372,%f3371,%f3370,%f3369,%f3368,%f3367,%f3366,%f3365,%f2486,%f2487,%f2488,%f2489,%f2490,%f2491,%f2492,%f2493,%f2494,%f2495
	wgmma.wait_group.sync.aligned 1;
	// end inline asm
	.loc	1 1205 54
	add.s32 	%r41, %r58, %r800;
	add.s32 	%r366, %r41, -1;
	mul.wide.s32 	%rd217, %r366, 4;
	add.s64 	%rd201, %rd17, %rd217;
	.loc	1 1205 16
	// begin inline asm
	mov.u32 %r353, 0x0;
	@%p55 ld.global.b32 { %r353 }, [ %rd201 + 0 ];
	// end inline asm
	mov.b32 	%f3230, %r353;
	.loc	1 1208 54
	add.s64 	%rd202, %rd18, %rd217;
	.loc	1 1208 16
	// begin inline asm
	mov.u32 %r354, 0x0;
	@%p55 ld.global.b32 { %r354 }, [ %rd202 + 0 ];
	// end inline asm
	mov.b32 	%f3231, %r354;
	.loc	1 1210 30
	mul.f32 	%f3494, %f3230, %f3231;
	.loc	1 1211 24
	setp.eq.s32 	%p78, %r25, %r800;
	.loc	1 1222 19
	mul.f32 	%f2698, %f2698, %f3364;
	mul.f32 	%f2699, %f2699, %f3364;
	mul.f32 	%f2700, %f2700, %f3364;
	mul.f32 	%f2701, %f2701, %f3364;
	mul.f32 	%f2702, %f2702, %f3364;
	mul.f32 	%f2703, %f2703, %f3364;
	mul.f32 	%f2704, %f2704, %f3364;
	mul.f32 	%f2705, %f2705, %f3364;
	mul.f32 	%f2706, %f2706, %f3364;
	mul.f32 	%f2707, %f2707, %f3364;
	mul.f32 	%f2708, %f2708, %f3364;
	mul.f32 	%f2709, %f2709, %f3364;
	mul.f32 	%f2710, %f2710, %f3364;
	mul.f32 	%f2711, %f2711, %f3364;
	mul.f32 	%f2712, %f2712, %f3364;
	mul.f32 	%f2713, %f2713, %f3364;
	mul.f32 	%f2714, %f2714, %f3364;
	mul.f32 	%f2715, %f2715, %f3364;
	mul.f32 	%f2716, %f2716, %f3364;
	mul.f32 	%f2717, %f2717, %f3364;
	mul.f32 	%f2718, %f2718, %f3364;
	mul.f32 	%f2719, %f2719, %f3364;
	mul.f32 	%f2720, %f2720, %f3364;
	mul.f32 	%f2721, %f2721, %f3364;
	mul.f32 	%f2722, %f2722, %f3364;
	mul.f32 	%f2723, %f2723, %f3364;
	mul.f32 	%f2724, %f2724, %f3364;
	mul.f32 	%f2725, %f2725, %f3364;
	mul.f32 	%f2726, %f2726, %f3364;
	mul.f32 	%f2727, %f2727, %f3364;
	mul.f32 	%f2728, %f2728, %f3364;
	mul.f32 	%f2729, %f2729, %f3364;
	mul.f32 	%f2730, %f2730, %f3364;
	mul.f32 	%f2731, %f2731, %f3364;
	mul.f32 	%f2732, %f2732, %f3364;
	mul.f32 	%f2733, %f2733, %f3364;
	mul.f32 	%f2734, %f2734, %f3364;
	mul.f32 	%f2735, %f2735, %f3364;
	mul.f32 	%f2736, %f2736, %f3364;
	mul.f32 	%f2737, %f2737, %f3364;
	mul.f32 	%f2738, %f2738, %f3364;
	mul.f32 	%f2739, %f2739, %f3364;
	mul.f32 	%f2740, %f2740, %f3364;
	mul.f32 	%f2741, %f2741, %f3364;
	mul.f32 	%f2742, %f2742, %f3364;
	mul.f32 	%f2743, %f2743, %f3364;
	mul.f32 	%f2744, %f2744, %f3364;
	mul.f32 	%f2745, %f2745, %f3364;
	mul.f32 	%f2746, %f2746, %f3364;
	mul.f32 	%f2747, %f2747, %f3364;
	mul.f32 	%f2748, %f2748, %f3364;
	mul.f32 	%f2749, %f2749, %f3364;
	mul.f32 	%f2750, %f2750, %f3364;
	mul.f32 	%f2751, %f2751, %f3364;
	mul.f32 	%f2752, %f2752, %f3364;
	mul.f32 	%f2753, %f2753, %f3364;
	mul.f32 	%f2754, %f2754, %f3364;
	mul.f32 	%f2755, %f2755, %f3364;
	mul.f32 	%f2756, %f2756, %f3364;
	mul.f32 	%f2757, %f2757, %f3364;
	mul.f32 	%f2758, %f2758, %f3364;
	mul.f32 	%f2759, %f2759, %f3364;
	mul.f32 	%f2760, %f2760, %f3364;
	mul.f32 	%f2761, %f2761, %f3364;
	.loc	1 1185 27
	shfl.sync.idx.b32	%r367, %r35, 0, 31, -1;
	// begin inline asm
	wgmma.fence.sync.aligned;
	// end inline asm
	shl.b32 	%r368, %r367, 7;
	and.b32  	%r369, %r368, 384;
	cvt.u64.u32 	%rd218, %r369;
	shr.u32 	%r370, %r359, 4;
	cvt.u64.u32 	%rd219, %r370;
	and.b64  	%rd220, %rd219, 16383;
	add.s64 	%rd221, %rd220, %rd218;
	or.b64  	%rd203, %rd221, 4611686293338849280;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761}, %rd203, %rd194, 1, 1, 1;
	// end inline asm
	add.s64 	%rd205, %rd221, 4611686293338849282;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761}, %rd205, %rd196, 1, 1, 1;
	// end inline asm
	add.s64 	%rd207, %rd221, 4611686293338849284;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761}, %rd207, %rd198, 1, 1, 1;
	// end inline asm
	add.s64 	%rd209, %rd221, 4611686293338849286;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n128k32.f32.e4m3.e4m3 {%f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761}, %rd209, %rd200, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned;
	// end inline asm
	mov.b32 	%f3146, %r359;
	mov.f32 	%f3148, %f1301;
	mov.f32 	%f3153, %f1300;
	mov.f32 	%f3150, %f1302;
	mov.f32 	%f3154, %f1302;
	mov.f32 	%f3155, %f1302;
	mov.f32 	%f3147, %f1300;
	mov.f32 	%f3149, %f1903;
	mov.f32 	%f3152, %f1301;
	// begin inline asm
	// wait for regs: %f2698,%f2699,%f2700,%f2701,%f2702,%f2703,%f2704,%f2705,%f2706,%f2707,%f2708,%f2709,%f2710,%f2711,%f2712,%f2713,%f2714,%f2715,%f2716,%f2717,%f2718,%f2719,%f2720,%f2721,%f2722,%f2723,%f2724,%f2725,%f2726,%f2727,%f2728,%f2729,%f2730,%f2731,%f2732,%f2733,%f2734,%f2735,%f2736,%f2737,%f2738,%f2739,%f2740,%f2741,%f2742,%f2743,%f2744,%f2745,%f2746,%f2747,%f2748,%f2749,%f2750,%f2751,%f2752,%f2753,%f2754,%f2755,%f2756,%f2757,%f2758,%f2759,%f2760,%f2761,%f3146,%f3147,%f3148,%f3149,%f3150,%f3151,%f3152,%f3153,%f3154,%f3155
	wgmma.wait_group.sync.aligned 1;
	// end inline asm
	.loc	1 1211 15
	@%p78 bra 	$L__BB0_6;
	.loc	1 1215 58
	mul.wide.s32 	%rd224, %r41, 4;
	add.s64 	%rd222, %rd17, %rd224;
	.loc	1 1215 20
	// begin inline asm
	mov.u32 %r371, 0x0;
	@%p55 ld.global.b32 { %r371 }, [ %rd222 + 0 ];
	// end inline asm
	mov.b32 	%f3232, %r371;
	.loc	1 1218 58
	add.s64 	%rd223, %rd18, %rd224;
	.loc	1 1218 20
	// begin inline asm
	mov.u32 %r372, 0x0;
	@%p55 ld.global.b32 { %r372 }, [ %rd223 + 0 ];
	// end inline asm
	mov.b32 	%f3233, %r372;
	.loc	1 1220 44
	mul.f32 	%f3234, %f3232, %f3233;
	.loc	1 1221 47
	mov.b32 	%r375, %f3234;
	mov.b32 	%r374, %f3494;
	// begin inline asm
	div.full.f32 %r373, %r374, %r375;
	// end inline asm
	mov.b32 	%f3494, %r373;
	bra.uni 	$L__BB0_6;
$L__BB0_7:
	.loc	1 1157 40
	shl.b32 	%r685, %r3, 3;
	and.b32  	%r686, %r685, 120;
	.loc	1 1158 27
	or.b32  	%r687, %r6, %r686;
	.loc	1 1157 40
	bfe.u32 	%r688, %r3, 4, 1;
	bfe.u32 	%r689, %r3, 4, 3;
	.loc	1 1157 27
	or.b32  	%r690, %r689, %r2;
	or.b32  	%r691, %r690, 56;
	or.b32  	%r692, %r690, 48;
	or.b32  	%r693, %r690, 40;
	or.b32  	%r694, %r690, 32;
	or.b32  	%r695, %r690, 24;
	or.b32  	%r696, %r690, 16;
	or.b32  	%r697, %r690, 8;
	.loc	1 1171 22
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 1222 19
	mul.f32 	%f3235, %f2698, %f3494;
	mul.f32 	%f3236, %f2699, %f3494;
	mul.f32 	%f3237, %f2700, %f3494;
	mul.f32 	%f3238, %f2701, %f3494;
	mul.f32 	%f3239, %f2702, %f3494;
	mul.f32 	%f3240, %f2703, %f3494;
	mul.f32 	%f3241, %f2704, %f3494;
	mul.f32 	%f3242, %f2705, %f3494;
	mul.f32 	%f3243, %f2706, %f3494;
	mul.f32 	%f3244, %f2707, %f3494;
	mul.f32 	%f3245, %f2708, %f3494;
	mul.f32 	%f3246, %f2709, %f3494;
	mul.f32 	%f3247, %f2710, %f3494;
	mul.f32 	%f3248, %f2711, %f3494;
	mul.f32 	%f3249, %f2712, %f3494;
	mul.f32 	%f3250, %f2713, %f3494;
	mul.f32 	%f3251, %f2714, %f3494;
	mul.f32 	%f3252, %f2715, %f3494;
	mul.f32 	%f3253, %f2716, %f3494;
	mul.f32 	%f3254, %f2717, %f3494;
	mul.f32 	%f3255, %f2718, %f3494;
	mul.f32 	%f3256, %f2719, %f3494;
	mul.f32 	%f3257, %f2720, %f3494;
	mul.f32 	%f3258, %f2721, %f3494;
	mul.f32 	%f3259, %f2722, %f3494;
	mul.f32 	%f3260, %f2723, %f3494;
	mul.f32 	%f3261, %f2724, %f3494;
	mul.f32 	%f3262, %f2725, %f3494;
	mul.f32 	%f3263, %f2726, %f3494;
	mul.f32 	%f3264, %f2727, %f3494;
	mul.f32 	%f3265, %f2728, %f3494;
	mul.f32 	%f3266, %f2729, %f3494;
	mul.f32 	%f3267, %f2730, %f3494;
	mul.f32 	%f3268, %f2731, %f3494;
	mul.f32 	%f3269, %f2732, %f3494;
	mul.f32 	%f3270, %f2733, %f3494;
	mul.f32 	%f3271, %f2734, %f3494;
	mul.f32 	%f3272, %f2735, %f3494;
	mul.f32 	%f3273, %f2736, %f3494;
	mul.f32 	%f3274, %f2737, %f3494;
	mul.f32 	%f3275, %f2738, %f3494;
	mul.f32 	%f3276, %f2739, %f3494;
	mul.f32 	%f3277, %f2740, %f3494;
	mul.f32 	%f3278, %f2741, %f3494;
	mul.f32 	%f3279, %f2742, %f3494;
	mul.f32 	%f3280, %f2743, %f3494;
	mul.f32 	%f3281, %f2744, %f3494;
	mul.f32 	%f3282, %f2745, %f3494;
	mul.f32 	%f3283, %f2746, %f3494;
	mul.f32 	%f3284, %f2747, %f3494;
	mul.f32 	%f3285, %f2748, %f3494;
	mul.f32 	%f3286, %f2749, %f3494;
	mul.f32 	%f3287, %f2750, %f3494;
	mul.f32 	%f3288, %f2751, %f3494;
	mul.f32 	%f3289, %f2752, %f3494;
	mul.f32 	%f3290, %f2753, %f3494;
	mul.f32 	%f3291, %f2754, %f3494;
	mul.f32 	%f3292, %f2755, %f3494;
	mul.f32 	%f3293, %f2756, %f3494;
	mul.f32 	%f3294, %f2757, %f3494;
	mul.f32 	%f3295, %f2758, %f3494;
	mul.f32 	%f3296, %f2759, %f3494;
	mul.f32 	%f3297, %f2760, %f3494;
	mul.f32 	%f3298, %f2761, %f3494;
	.loc	1 1228 17
	mov.b32 	%r413, %f3428;
	// begin inline asm
	cvt.rn.bf16.f32 %rs1, %r413;
	// end inline asm
	mov.b32 	%r414, %f3427;
	// begin inline asm
	cvt.rn.bf16.f32 %rs2, %r414;
	// end inline asm
	mov.b32 	%r415, %f3426;
	// begin inline asm
	cvt.rn.bf16.f32 %rs3, %r415;
	// end inline asm
	mov.b32 	%r416, %f3425;
	// begin inline asm
	cvt.rn.bf16.f32 %rs4, %r416;
	// end inline asm
	mov.b32 	%r417, %f3424;
	// begin inline asm
	cvt.rn.bf16.f32 %rs5, %r417;
	// end inline asm
	mov.b32 	%r418, %f3423;
	// begin inline asm
	cvt.rn.bf16.f32 %rs6, %r418;
	// end inline asm
	mov.b32 	%r419, %f3422;
	// begin inline asm
	cvt.rn.bf16.f32 %rs7, %r419;
	// end inline asm
	mov.b32 	%r420, %f3421;
	// begin inline asm
	cvt.rn.bf16.f32 %rs8, %r420;
	// end inline asm
	mov.b32 	%r421, %f3420;
	// begin inline asm
	cvt.rn.bf16.f32 %rs9, %r421;
	// end inline asm
	mov.b32 	%r422, %f3419;
	// begin inline asm
	cvt.rn.bf16.f32 %rs10, %r422;
	// end inline asm
	mov.b32 	%r423, %f3418;
	// begin inline asm
	cvt.rn.bf16.f32 %rs11, %r423;
	// end inline asm
	mov.b32 	%r424, %f3417;
	// begin inline asm
	cvt.rn.bf16.f32 %rs12, %r424;
	// end inline asm
	mov.b32 	%r425, %f3416;
	// begin inline asm
	cvt.rn.bf16.f32 %rs13, %r425;
	// end inline asm
	mov.b32 	%r426, %f3415;
	// begin inline asm
	cvt.rn.bf16.f32 %rs14, %r426;
	// end inline asm
	mov.b32 	%r427, %f3414;
	// begin inline asm
	cvt.rn.bf16.f32 %rs15, %r427;
	// end inline asm
	mov.b32 	%r428, %f3413;
	// begin inline asm
	cvt.rn.bf16.f32 %rs16, %r428;
	// end inline asm
	mov.b32 	%r429, %f3412;
	// begin inline asm
	cvt.rn.bf16.f32 %rs17, %r429;
	// end inline asm
	mov.b32 	%r430, %f3411;
	// begin inline asm
	cvt.rn.bf16.f32 %rs18, %r430;
	// end inline asm
	mov.b32 	%r431, %f3410;
	// begin inline asm
	cvt.rn.bf16.f32 %rs19, %r431;
	// end inline asm
	mov.b32 	%r432, %f3409;
	// begin inline asm
	cvt.rn.bf16.f32 %rs20, %r432;
	// end inline asm
	mov.b32 	%r433, %f3408;
	// begin inline asm
	cvt.rn.bf16.f32 %rs21, %r433;
	// end inline asm
	mov.b32 	%r434, %f3407;
	// begin inline asm
	cvt.rn.bf16.f32 %rs22, %r434;
	// end inline asm
	mov.b32 	%r435, %f3406;
	// begin inline asm
	cvt.rn.bf16.f32 %rs23, %r435;
	// end inline asm
	mov.b32 	%r436, %f3405;
	// begin inline asm
	cvt.rn.bf16.f32 %rs24, %r436;
	// end inline asm
	mov.b32 	%r437, %f3404;
	// begin inline asm
	cvt.rn.bf16.f32 %rs25, %r437;
	// end inline asm
	mov.b32 	%r438, %f3403;
	// begin inline asm
	cvt.rn.bf16.f32 %rs26, %r438;
	// end inline asm
	mov.b32 	%r439, %f3402;
	// begin inline asm
	cvt.rn.bf16.f32 %rs27, %r439;
	// end inline asm
	mov.b32 	%r440, %f3401;
	// begin inline asm
	cvt.rn.bf16.f32 %rs28, %r440;
	// end inline asm
	mov.b32 	%r441, %f3400;
	// begin inline asm
	cvt.rn.bf16.f32 %rs29, %r441;
	// end inline asm
	mov.b32 	%r442, %f3399;
	// begin inline asm
	cvt.rn.bf16.f32 %rs30, %r442;
	// end inline asm
	mov.b32 	%r443, %f3398;
	// begin inline asm
	cvt.rn.bf16.f32 %rs31, %r443;
	// end inline asm
	mov.b32 	%r444, %f3397;
	// begin inline asm
	cvt.rn.bf16.f32 %rs32, %r444;
	// end inline asm
	mov.b32 	%r445, %f3396;
	// begin inline asm
	cvt.rn.bf16.f32 %rs33, %r445;
	// end inline asm
	mov.b32 	%r446, %f3395;
	// begin inline asm
	cvt.rn.bf16.f32 %rs34, %r446;
	// end inline asm
	mov.b32 	%r447, %f3394;
	// begin inline asm
	cvt.rn.bf16.f32 %rs35, %r447;
	// end inline asm
	mov.b32 	%r448, %f3393;
	// begin inline asm
	cvt.rn.bf16.f32 %rs36, %r448;
	// end inline asm
	mov.b32 	%r449, %f3392;
	// begin inline asm
	cvt.rn.bf16.f32 %rs37, %r449;
	// end inline asm
	mov.b32 	%r450, %f3391;
	// begin inline asm
	cvt.rn.bf16.f32 %rs38, %r450;
	// end inline asm
	mov.b32 	%r451, %f3390;
	// begin inline asm
	cvt.rn.bf16.f32 %rs39, %r451;
	// end inline asm
	mov.b32 	%r452, %f3389;
	// begin inline asm
	cvt.rn.bf16.f32 %rs40, %r452;
	// end inline asm
	mov.b32 	%r453, %f3388;
	// begin inline asm
	cvt.rn.bf16.f32 %rs41, %r453;
	// end inline asm
	mov.b32 	%r454, %f3387;
	// begin inline asm
	cvt.rn.bf16.f32 %rs42, %r454;
	// end inline asm
	mov.b32 	%r455, %f3386;
	// begin inline asm
	cvt.rn.bf16.f32 %rs43, %r455;
	// end inline asm
	mov.b32 	%r456, %f3385;
	// begin inline asm
	cvt.rn.bf16.f32 %rs44, %r456;
	// end inline asm
	mov.b32 	%r457, %f3384;
	// begin inline asm
	cvt.rn.bf16.f32 %rs45, %r457;
	// end inline asm
	mov.b32 	%r458, %f3383;
	// begin inline asm
	cvt.rn.bf16.f32 %rs46, %r458;
	// end inline asm
	mov.b32 	%r459, %f3382;
	// begin inline asm
	cvt.rn.bf16.f32 %rs47, %r459;
	// end inline asm
	mov.b32 	%r460, %f3381;
	// begin inline asm
	cvt.rn.bf16.f32 %rs48, %r460;
	// end inline asm
	mov.b32 	%r461, %f3380;
	// begin inline asm
	cvt.rn.bf16.f32 %rs49, %r461;
	// end inline asm
	mov.b32 	%r462, %f3379;
	// begin inline asm
	cvt.rn.bf16.f32 %rs50, %r462;
	// end inline asm
	mov.b32 	%r463, %f3378;
	// begin inline asm
	cvt.rn.bf16.f32 %rs51, %r463;
	// end inline asm
	mov.b32 	%r464, %f3377;
	// begin inline asm
	cvt.rn.bf16.f32 %rs52, %r464;
	// end inline asm
	mov.b32 	%r465, %f3376;
	// begin inline asm
	cvt.rn.bf16.f32 %rs53, %r465;
	// end inline asm
	mov.b32 	%r466, %f3375;
	// begin inline asm
	cvt.rn.bf16.f32 %rs54, %r466;
	// end inline asm
	mov.b32 	%r467, %f3374;
	// begin inline asm
	cvt.rn.bf16.f32 %rs55, %r467;
	// end inline asm
	mov.b32 	%r468, %f3373;
	// begin inline asm
	cvt.rn.bf16.f32 %rs56, %r468;
	// end inline asm
	mov.b32 	%r469, %f3372;
	// begin inline asm
	cvt.rn.bf16.f32 %rs57, %r469;
	// end inline asm
	mov.b32 	%r470, %f3371;
	// begin inline asm
	cvt.rn.bf16.f32 %rs58, %r470;
	// end inline asm
	mov.b32 	%r471, %f3370;
	// begin inline asm
	cvt.rn.bf16.f32 %rs59, %r471;
	// end inline asm
	mov.b32 	%r472, %f3369;
	// begin inline asm
	cvt.rn.bf16.f32 %rs60, %r472;
	// end inline asm
	mov.b32 	%r473, %f3368;
	// begin inline asm
	cvt.rn.bf16.f32 %rs61, %r473;
	// end inline asm
	mov.b32 	%r474, %f3367;
	// begin inline asm
	cvt.rn.bf16.f32 %rs62, %r474;
	// end inline asm
	mov.b32 	%r475, %f3366;
	// begin inline asm
	cvt.rn.bf16.f32 %rs63, %r475;
	// end inline asm
	mov.b32 	%r476, %f3365;
	// begin inline asm
	cvt.rn.bf16.f32 %rs64, %r476;
	// end inline asm
	mov.b32 	%r477, %f3235;
	// begin inline asm
	cvt.rn.bf16.f32 %rs65, %r477;
	// end inline asm
	mov.b32 	%r478, %f3236;
	// begin inline asm
	cvt.rn.bf16.f32 %rs66, %r478;
	// end inline asm
	mov.b32 	%r479, %f3237;
	// begin inline asm
	cvt.rn.bf16.f32 %rs67, %r479;
	// end inline asm
	mov.b32 	%r480, %f3238;
	// begin inline asm
	cvt.rn.bf16.f32 %rs68, %r480;
	// end inline asm
	mov.b32 	%r481, %f3239;
	// begin inline asm
	cvt.rn.bf16.f32 %rs69, %r481;
	// end inline asm
	mov.b32 	%r482, %f3240;
	// begin inline asm
	cvt.rn.bf16.f32 %rs70, %r482;
	// end inline asm
	mov.b32 	%r483, %f3241;
	// begin inline asm
	cvt.rn.bf16.f32 %rs71, %r483;
	// end inline asm
	mov.b32 	%r484, %f3242;
	// begin inline asm
	cvt.rn.bf16.f32 %rs72, %r484;
	// end inline asm
	mov.b32 	%r485, %f3243;
	// begin inline asm
	cvt.rn.bf16.f32 %rs73, %r485;
	// end inline asm
	mov.b32 	%r486, %f3244;
	// begin inline asm
	cvt.rn.bf16.f32 %rs74, %r486;
	// end inline asm
	mov.b32 	%r487, %f3245;
	// begin inline asm
	cvt.rn.bf16.f32 %rs75, %r487;
	// end inline asm
	mov.b32 	%r488, %f3246;
	// begin inline asm
	cvt.rn.bf16.f32 %rs76, %r488;
	// end inline asm
	mov.b32 	%r489, %f3247;
	// begin inline asm
	cvt.rn.bf16.f32 %rs77, %r489;
	// end inline asm
	mov.b32 	%r490, %f3248;
	// begin inline asm
	cvt.rn.bf16.f32 %rs78, %r490;
	// end inline asm
	mov.b32 	%r491, %f3249;
	// begin inline asm
	cvt.rn.bf16.f32 %rs79, %r491;
	// end inline asm
	mov.b32 	%r492, %f3250;
	// begin inline asm
	cvt.rn.bf16.f32 %rs80, %r492;
	// end inline asm
	mov.b32 	%r493, %f3251;
	// begin inline asm
	cvt.rn.bf16.f32 %rs81, %r493;
	// end inline asm
	mov.b32 	%r494, %f3252;
	// begin inline asm
	cvt.rn.bf16.f32 %rs82, %r494;
	// end inline asm
	mov.b32 	%r495, %f3253;
	// begin inline asm
	cvt.rn.bf16.f32 %rs83, %r495;
	// end inline asm
	mov.b32 	%r496, %f3254;
	// begin inline asm
	cvt.rn.bf16.f32 %rs84, %r496;
	// end inline asm
	mov.b32 	%r497, %f3255;
	// begin inline asm
	cvt.rn.bf16.f32 %rs85, %r497;
	// end inline asm
	mov.b32 	%r498, %f3256;
	// begin inline asm
	cvt.rn.bf16.f32 %rs86, %r498;
	// end inline asm
	mov.b32 	%r499, %f3257;
	// begin inline asm
	cvt.rn.bf16.f32 %rs87, %r499;
	// end inline asm
	mov.b32 	%r500, %f3258;
	// begin inline asm
	cvt.rn.bf16.f32 %rs88, %r500;
	// end inline asm
	mov.b32 	%r501, %f3259;
	// begin inline asm
	cvt.rn.bf16.f32 %rs89, %r501;
	// end inline asm
	mov.b32 	%r502, %f3260;
	// begin inline asm
	cvt.rn.bf16.f32 %rs90, %r502;
	// end inline asm
	mov.b32 	%r503, %f3261;
	// begin inline asm
	cvt.rn.bf16.f32 %rs91, %r503;
	// end inline asm
	mov.b32 	%r504, %f3262;
	// begin inline asm
	cvt.rn.bf16.f32 %rs92, %r504;
	// end inline asm
	mov.b32 	%r505, %f3263;
	// begin inline asm
	cvt.rn.bf16.f32 %rs93, %r505;
	// end inline asm
	mov.b32 	%r506, %f3264;
	// begin inline asm
	cvt.rn.bf16.f32 %rs94, %r506;
	// end inline asm
	mov.b32 	%r507, %f3265;
	// begin inline asm
	cvt.rn.bf16.f32 %rs95, %r507;
	// end inline asm
	mov.b32 	%r508, %f3266;
	// begin inline asm
	cvt.rn.bf16.f32 %rs96, %r508;
	// end inline asm
	mov.b32 	%r509, %f3267;
	// begin inline asm
	cvt.rn.bf16.f32 %rs97, %r509;
	// end inline asm
	mov.b32 	%r510, %f3268;
	// begin inline asm
	cvt.rn.bf16.f32 %rs98, %r510;
	// end inline asm
	mov.b32 	%r511, %f3269;
	// begin inline asm
	cvt.rn.bf16.f32 %rs99, %r511;
	// end inline asm
	mov.b32 	%r512, %f3270;
	// begin inline asm
	cvt.rn.bf16.f32 %rs100, %r512;
	// end inline asm
	mov.b32 	%r513, %f3271;
	// begin inline asm
	cvt.rn.bf16.f32 %rs101, %r513;
	// end inline asm
	mov.b32 	%r514, %f3272;
	// begin inline asm
	cvt.rn.bf16.f32 %rs102, %r514;
	// end inline asm
	mov.b32 	%r515, %f3273;
	// begin inline asm
	cvt.rn.bf16.f32 %rs103, %r515;
	// end inline asm
	mov.b32 	%r516, %f3274;
	// begin inline asm
	cvt.rn.bf16.f32 %rs104, %r516;
	// end inline asm
	mov.b32 	%r517, %f3275;
	// begin inline asm
	cvt.rn.bf16.f32 %rs105, %r517;
	// end inline asm
	mov.b32 	%r518, %f3276;
	// begin inline asm
	cvt.rn.bf16.f32 %rs106, %r518;
	// end inline asm
	mov.b32 	%r519, %f3277;
	// begin inline asm
	cvt.rn.bf16.f32 %rs107, %r519;
	// end inline asm
	mov.b32 	%r520, %f3278;
	// begin inline asm
	cvt.rn.bf16.f32 %rs108, %r520;
	// end inline asm
	mov.b32 	%r521, %f3279;
	// begin inline asm
	cvt.rn.bf16.f32 %rs109, %r521;
	// end inline asm
	mov.b32 	%r522, %f3280;
	// begin inline asm
	cvt.rn.bf16.f32 %rs110, %r522;
	// end inline asm
	mov.b32 	%r523, %f3281;
	// begin inline asm
	cvt.rn.bf16.f32 %rs111, %r523;
	// end inline asm
	mov.b32 	%r524, %f3282;
	// begin inline asm
	cvt.rn.bf16.f32 %rs112, %r524;
	// end inline asm
	mov.b32 	%r525, %f3283;
	// begin inline asm
	cvt.rn.bf16.f32 %rs113, %r525;
	// end inline asm
	mov.b32 	%r526, %f3284;
	// begin inline asm
	cvt.rn.bf16.f32 %rs114, %r526;
	// end inline asm
	mov.b32 	%r527, %f3285;
	// begin inline asm
	cvt.rn.bf16.f32 %rs115, %r527;
	// end inline asm
	mov.b32 	%r528, %f3286;
	// begin inline asm
	cvt.rn.bf16.f32 %rs116, %r528;
	// end inline asm
	mov.b32 	%r529, %f3287;
	// begin inline asm
	cvt.rn.bf16.f32 %rs117, %r529;
	// end inline asm
	mov.b32 	%r530, %f3288;
	// begin inline asm
	cvt.rn.bf16.f32 %rs118, %r530;
	// end inline asm
	mov.b32 	%r531, %f3289;
	// begin inline asm
	cvt.rn.bf16.f32 %rs119, %r531;
	// end inline asm
	mov.b32 	%r532, %f3290;
	// begin inline asm
	cvt.rn.bf16.f32 %rs120, %r532;
	// end inline asm
	mov.b32 	%r533, %f3291;
	// begin inline asm
	cvt.rn.bf16.f32 %rs121, %r533;
	// end inline asm
	mov.b32 	%r534, %f3292;
	// begin inline asm
	cvt.rn.bf16.f32 %rs122, %r534;
	// end inline asm
	mov.b32 	%r535, %f3293;
	// begin inline asm
	cvt.rn.bf16.f32 %rs123, %r535;
	// end inline asm
	mov.b32 	%r536, %f3294;
	// begin inline asm
	cvt.rn.bf16.f32 %rs124, %r536;
	// end inline asm
	mov.b32 	%r537, %f3295;
	// begin inline asm
	cvt.rn.bf16.f32 %rs125, %r537;
	// end inline asm
	mov.b32 	%r538, %f3296;
	// begin inline asm
	cvt.rn.bf16.f32 %rs126, %r538;
	// end inline asm
	mov.b32 	%r539, %f3297;
	// begin inline asm
	cvt.rn.bf16.f32 %rs127, %r539;
	// end inline asm
	mov.b32 	%r540, %f3298;
	// begin inline asm
	cvt.rn.bf16.f32 %rs128, %r540;
	// end inline asm
	.loc	1 1229 27
	shl.b32 	%r698, %r56, 3;
	.loc	1 1229 39
	mad.lo.s32 	%r699, %r690, %r56, %r687;
	add.s32 	%r700, %r699, %r698;
	add.s32 	%r701, %r700, %r698;
	add.s32 	%r702, %r701, %r698;
	add.s32 	%r703, %r702, %r698;
	add.s32 	%r704, %r703, %r698;
	add.s32 	%r705, %r704, %r698;
	add.s32 	%r706, %r705, %r698;
	.loc	1 1229 13
	mul.wide.s32 	%rd257, %r699, 2;
	add.s64 	%rd241, %rd43, %rd257;
	mul.wide.s32 	%rd258, %r700, 2;
	add.s64 	%rd242, %rd43, %rd258;
	mul.wide.s32 	%rd259, %r701, 2;
	add.s64 	%rd243, %rd43, %rd259;
	mul.wide.s32 	%rd260, %r702, 2;
	add.s64 	%rd244, %rd43, %rd260;
	mul.wide.s32 	%rd261, %r703, 2;
	add.s64 	%rd245, %rd43, %rd261;
	mul.wide.s32 	%rd262, %r704, 2;
	add.s64 	%rd246, %rd43, %rd262;
	mul.wide.s32 	%rd263, %r705, 2;
	add.s64 	%rd247, %rd43, %rd263;
	mul.wide.s32 	%rd264, %r706, 2;
	add.s64 	%rd248, %rd43, %rd264;
	.loc	1 1157 27
	or.b32  	%r707, %r690, 64;
	or.b32  	%r708, %r690, 72;
	or.b32  	%r709, %r690, 80;
	or.b32  	%r710, %r690, 88;
	or.b32  	%r711, %r690, 96;
	or.b32  	%r712, %r690, 104;
	or.b32  	%r713, %r690, 112;
	or.b32  	%r714, %r690, 120;
	.loc	1 1229 39
	add.s32 	%r715, %r706, %r698;
	add.s32 	%r716, %r715, %r698;
	add.s32 	%r717, %r716, %r698;
	add.s32 	%r718, %r717, %r698;
	add.s32 	%r719, %r718, %r698;
	add.s32 	%r720, %r719, %r698;
	add.s32 	%r721, %r720, %r698;
	add.s32 	%r722, %r721, %r698;
	.loc	1 1229 13
	mul.wide.s32 	%rd265, %r715, 2;
	add.s64 	%rd249, %rd43, %rd265;
	mul.wide.s32 	%rd266, %r716, 2;
	add.s64 	%rd250, %rd43, %rd266;
	mul.wide.s32 	%rd267, %r717, 2;
	add.s64 	%rd251, %rd43, %rd267;
	mul.wide.s32 	%rd268, %r718, 2;
	add.s64 	%rd252, %rd43, %rd268;
	mul.wide.s32 	%rd269, %r719, 2;
	add.s64 	%rd253, %rd43, %rd269;
	mul.wide.s32 	%rd270, %r720, 2;
	add.s64 	%rd254, %rd43, %rd270;
	mul.wide.s32 	%rd271, %r721, 2;
	add.s64 	%rd255, %rd43, %rd271;
	mul.wide.s32 	%rd272, %r722, 2;
	add.s64 	%rd256, %rd43, %rd272;
	.loc	1 1230 17
	setp.lt.s32 	%p116, %r690, %r54;
	setp.lt.s32 	%p117, %r697, %r54;
	setp.lt.s32 	%p118, %r696, %r54;
	setp.lt.s32 	%p119, %r695, %r54;
	setp.lt.s32 	%p120, %r694, %r54;
	setp.lt.s32 	%p121, %r693, %r54;
	setp.lt.s32 	%p122, %r692, %r54;
	setp.lt.s32 	%p123, %r691, %r54;
	.loc	1 1230 37
	setp.lt.s32 	%p124, %r687, %r55;
	.loc	1 1230 31
	and.pred  	%p100, %p116, %p124;
	and.pred  	%p101, %p117, %p124;
	and.pred  	%p102, %p118, %p124;
	and.pred  	%p103, %p119, %p124;
	and.pred  	%p104, %p120, %p124;
	and.pred  	%p105, %p121, %p124;
	and.pred  	%p106, %p122, %p124;
	and.pred  	%p107, %p123, %p124;
	.loc	1 1233 20
	and.b32  	%r723, %r4, 3;
	and.b32  	%r724, %r3, 15;
	shr.u32 	%r725, %r3, 1;
	and.b32  	%r726, %r725, 8;
	mad.lo.s32 	%r727, %r724, 136, %r726;
	mad.lo.s32 	%r728, %r723, 2176, %r727;
	mov.b32 	%r729, {%rs1, %rs2};
	mov.b32 	%r730, {%rs3, %rs4};
	mov.b32 	%r731, {%rs5, %rs6};
	mov.b32 	%r732, {%rs7, %rs8};
	shl.b32 	%r733, %r728, 1;
	add.s32 	%r541, %r252, %r733;
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r541], {%r729, %r730, %r731, %r732};
	// end inline asm
	mov.b32 	%r735, {%rs9, %rs10};
	mov.b32 	%r736, {%rs11, %rs12};
	mov.b32 	%r737, {%rs13, %rs14};
	mov.b32 	%r738, {%rs15, %rs16};
	add.s32 	%r546, %r541, 32;
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r546], {%r735, %r736, %r737, %r738};
	// end inline asm
	mov.b32 	%r739, {%rs17, %rs18};
	mov.b32 	%r740, {%rs19, %rs20};
	mov.b32 	%r741, {%rs21, %rs22};
	mov.b32 	%r742, {%rs23, %rs24};
	add.s32 	%r551, %r541, 64;
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r551], {%r739, %r740, %r741, %r742};
	// end inline asm
	mov.b32 	%r743, {%rs25, %rs26};
	mov.b32 	%r744, {%rs27, %rs28};
	mov.b32 	%r745, {%rs29, %rs30};
	mov.b32 	%r746, {%rs31, %rs32};
	add.s32 	%r556, %r541, 96;
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r556], {%r743, %r744, %r745, %r746};
	// end inline asm
	mov.b32 	%r747, {%rs33, %rs34};
	mov.b32 	%r748, {%rs35, %rs36};
	mov.b32 	%r749, {%rs37, %rs38};
	mov.b32 	%r750, {%rs39, %rs40};
	add.s32 	%r561, %r541, 128;
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r561], {%r747, %r748, %r749, %r750};
	// end inline asm
	mov.b32 	%r751, {%rs41, %rs42};
	mov.b32 	%r752, {%rs43, %rs44};
	mov.b32 	%r753, {%rs45, %rs46};
	mov.b32 	%r754, {%rs47, %rs48};
	add.s32 	%r566, %r541, 160;
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r566], {%r751, %r752, %r753, %r754};
	// end inline asm
	mov.b32 	%r755, {%rs49, %rs50};
	mov.b32 	%r756, {%rs51, %rs52};
	mov.b32 	%r757, {%rs53, %rs54};
	mov.b32 	%r758, {%rs55, %rs56};
	add.s32 	%r571, %r541, 192;
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r571], {%r755, %r756, %r757, %r758};
	// end inline asm
	mov.b32 	%r759, {%rs57, %rs58};
	mov.b32 	%r760, {%rs59, %rs60};
	mov.b32 	%r761, {%rs61, %rs62};
	mov.b32 	%r762, {%rs63, %rs64};
	add.s32 	%r576, %r541, 224;
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r576], {%r759, %r760, %r761, %r762};
	// end inline asm
	bar.sync 	0;
	shl.b32 	%r763, %r723, 1;
	or.b32  	%r764, %r763, %r688;
	mad.lo.s32 	%r765, %r764, 136, %r686;
	shl.b32 	%r766, %r765, 1;
	add.s32 	%r767, %r252, %r766;
	ld.shared.v4.u32 	{%r621, %r622, %r623, %r624}, [%r767];
	ld.shared.v4.u32 	{%r625, %r626, %r627, %r628}, [%r767+2176];
	ld.shared.v4.u32 	{%r629, %r630, %r631, %r632}, [%r767+4352];
	ld.shared.v4.u32 	{%r633, %r634, %r635, %r636}, [%r767+6528];
	ld.shared.v4.u32 	{%r637, %r638, %r639, %r640}, [%r767+8704];
	ld.shared.v4.u32 	{%r641, %r642, %r643, %r644}, [%r767+10880];
	ld.shared.v4.u32 	{%r645, %r646, %r647, %r648}, [%r767+13056];
	ld.shared.v4.u32 	{%r649, %r650, %r651, %r652}, [%r767+15232];
	bar.sync 	0;
	mov.b32 	%r768, {%rs65, %rs66};
	mov.b32 	%r769, {%rs67, %rs68};
	mov.b32 	%r770, {%rs69, %rs70};
	mov.b32 	%r771, {%rs71, %rs72};
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r541], {%r768, %r769, %r770, %r771};
	// end inline asm
	mov.b32 	%r772, {%rs73, %rs74};
	mov.b32 	%r773, {%rs75, %rs76};
	mov.b32 	%r774, {%rs77, %rs78};
	mov.b32 	%r775, {%rs79, %rs80};
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r546], {%r772, %r773, %r774, %r775};
	// end inline asm
	mov.b32 	%r776, {%rs81, %rs82};
	mov.b32 	%r777, {%rs83, %rs84};
	mov.b32 	%r778, {%rs85, %rs86};
	mov.b32 	%r779, {%rs87, %rs88};
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r551], {%r776, %r777, %r778, %r779};
	// end inline asm
	mov.b32 	%r780, {%rs89, %rs90};
	mov.b32 	%r781, {%rs91, %rs92};
	mov.b32 	%r782, {%rs93, %rs94};
	mov.b32 	%r783, {%rs95, %rs96};
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r556], {%r780, %r781, %r782, %r783};
	// end inline asm
	mov.b32 	%r784, {%rs97, %rs98};
	mov.b32 	%r785, {%rs99, %rs100};
	mov.b32 	%r786, {%rs101, %rs102};
	mov.b32 	%r787, {%rs103, %rs104};
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r561], {%r784, %r785, %r786, %r787};
	// end inline asm
	mov.b32 	%r788, {%rs105, %rs106};
	mov.b32 	%r789, {%rs107, %rs108};
	mov.b32 	%r790, {%rs109, %rs110};
	mov.b32 	%r791, {%rs111, %rs112};
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r566], {%r788, %r789, %r790, %r791};
	// end inline asm
	mov.b32 	%r792, {%rs113, %rs114};
	mov.b32 	%r793, {%rs115, %rs116};
	mov.b32 	%r794, {%rs117, %rs118};
	mov.b32 	%r795, {%rs119, %rs120};
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r571], {%r792, %r793, %r794, %r795};
	// end inline asm
	mov.b32 	%r796, {%rs121, %rs122};
	mov.b32 	%r797, {%rs123, %rs124};
	mov.b32 	%r798, {%rs125, %rs126};
	mov.b32 	%r799, {%rs127, %rs128};
	// begin inline asm
	stmatrix.sync.aligned.m8n8.x4.shared.b16 [%r576], {%r796, %r797, %r798, %r799};
	// end inline asm
	bar.sync 	0;
	ld.shared.v4.u32 	{%r653, %r654, %r655, %r656}, [%r767];
	ld.shared.v4.u32 	{%r657, %r658, %r659, %r660}, [%r767+2176];
	ld.shared.v4.u32 	{%r661, %r662, %r663, %r664}, [%r767+4352];
	ld.shared.v4.u32 	{%r665, %r666, %r667, %r668}, [%r767+6528];
	ld.shared.v4.u32 	{%r669, %r670, %r671, %r672}, [%r767+8704];
	ld.shared.v4.u32 	{%r673, %r674, %r675, %r676}, [%r767+10880];
	ld.shared.v4.u32 	{%r677, %r678, %r679, %r680}, [%r767+13056];
	ld.shared.v4.u32 	{%r681, %r682, %r683, %r684}, [%r767+15232];
	// begin inline asm
	@%p100 st.global.v4.b32 [ %rd241 + 0 ], { %r621, %r622, %r623, %r624 };
	// end inline asm
	// begin inline asm
	@%p101 st.global.v4.b32 [ %rd242 + 0 ], { %r625, %r626, %r627, %r628 };
	// end inline asm
	// begin inline asm
	@%p102 st.global.v4.b32 [ %rd243 + 0 ], { %r629, %r630, %r631, %r632 };
	// end inline asm
	// begin inline asm
	@%p103 st.global.v4.b32 [ %rd244 + 0 ], { %r633, %r634, %r635, %r636 };
	// end inline asm
	// begin inline asm
	@%p104 st.global.v4.b32 [ %rd245 + 0 ], { %r637, %r638, %r639, %r640 };
	// end inline asm
	// begin inline asm
	@%p105 st.global.v4.b32 [ %rd246 + 0 ], { %r641, %r642, %r643, %r644 };
	// end inline asm
	// begin inline asm
	@%p106 st.global.v4.b32 [ %rd247 + 0 ], { %r645, %r646, %r647, %r648 };
	// end inline asm
	// begin inline asm
	@%p107 st.global.v4.b32 [ %rd248 + 0 ], { %r649, %r650, %r651, %r652 };
	// end inline asm
	.loc	1 276 13
	setp.lt.s32 	%p125, %r707, %r54;
	setp.lt.s32 	%p126, %r708, %r54;
	setp.lt.s32 	%p127, %r709, %r54;
	setp.lt.s32 	%p128, %r710, %r54;
	setp.lt.s32 	%p129, %r711, %r54;
	setp.lt.s32 	%p130, %r712, %r54;
	setp.lt.s32 	%p131, %r713, %r54;
	setp.lt.s32 	%p132, %r714, %r54;
	.loc	1 279 13
	and.pred  	%p108, %p125, %p124;
	and.pred  	%p109, %p126, %p124;
	and.pred  	%p110, %p127, %p124;
	and.pred  	%p111, %p128, %p124;
	and.pred  	%p112, %p129, %p124;
	and.pred  	%p113, %p130, %p124;
	and.pred  	%p114, %p131, %p124;
	and.pred  	%p115, %p132, %p124;
	.loc	1 1233 20
	// begin inline asm
	@%p108 st.global.v4.b32 [ %rd249 + 0 ], { %r653, %r654, %r655, %r656 };
	// end inline asm
	// begin inline asm
	@%p109 st.global.v4.b32 [ %rd250 + 0 ], { %r657, %r658, %r659, %r660 };
	// end inline asm
	// begin inline asm
	@%p110 st.global.v4.b32 [ %rd251 + 0 ], { %r661, %r662, %r663, %r664 };
	// end inline asm
	// begin inline asm
	@%p111 st.global.v4.b32 [ %rd252 + 0 ], { %r665, %r666, %r667, %r668 };
	// end inline asm
	// begin inline asm
	@%p112 st.global.v4.b32 [ %rd253 + 0 ], { %r669, %r670, %r671, %r672 };
	// end inline asm
	// begin inline asm
	@%p113 st.global.v4.b32 [ %rd254 + 0 ], { %r673, %r674, %r675, %r676 };
	// end inline asm
	// begin inline asm
	@%p114 st.global.v4.b32 [ %rd255 + 0 ], { %r677, %r678, %r679, %r680 };
	// end inline asm
	// begin inline asm
	@%p115 st.global.v4.b32 [ %rd256 + 0 ], { %r681, %r682, %r683, %r684 };
	// end inline asm
	.loc	1 1232 4
	ret;
$L__tmp6:
$L__func_end0:

}
	.file	1 "/home/hoy/local/fbsource/buck-out/v2/gen/fbcode/f1c834dd0becaed6/deeplearning/fbgemm/fbgemm_gpu/experimental/gen_ai/bench/__quantize_bench__/quantize_bench#link-tree/fbgemm_gpu/experimental/gemm/triton_gemm/fp8_gemm.py"
	.file	2 "/home/hoy/local/fbsource/buck-out/v2/gen/fbcode/f1c834dd0becaed6/deeplearning/fbgemm/fbgemm_gpu/experimental/gen_ai/bench/__quantize_bench__/quantize_bench#link-tree/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 5
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 389
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 102
.b8 112
.b8 56
.b8 95
.b8 103
.b8 101
.b8 109
.b8 109
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 104
.b8 111
.b8 121
.b8 47
.b8 108
.b8 111
.b8 99
.b8 97
.b8 108
.b8 47
.b8 102
.b8 98
.b8 115
.b8 111
.b8 117
.b8 114
.b8 99
.b8 101
.b8 47
.b8 98
.b8 117
.b8 99
.b8 107
.b8 45
.b8 111
.b8 117
.b8 116
.b8 47
.b8 118
.b8 50
.b8 47
.b8 103
.b8 101
.b8 110
.b8 47
.b8 102
.b8 98
.b8 99
.b8 111
.b8 100
.b8 101
.b8 47
.b8 102
.b8 49
.b8 99
.b8 56
.b8 51
.b8 52
.b8 100
.b8 100
.b8 48
.b8 98
.b8 101
.b8 99
.b8 97
.b8 101
.b8 100
.b8 54
.b8 47
.b8 100
.b8 101
.b8 101
.b8 112
.b8 108
.b8 101
.b8 97
.b8 114
.b8 110
.b8 105
.b8 110
.b8 103
.b8 47
.b8 102
.b8 98
.b8 103
.b8 101
.b8 109
.b8 109
.b8 47
.b8 102
.b8 98
.b8 103
.b8 101
.b8 109
.b8 109
.b8 95
.b8 103
.b8 112
.b8 117
.b8 47
.b8 101
.b8 120
.b8 112
.b8 101
.b8 114
.b8 105
.b8 109
.b8 101
.b8 110
.b8 116
.b8 97
.b8 108
.b8 47
.b8 103
.b8 101
.b8 110
.b8 95
.b8 97
.b8 105
.b8 47
.b8 98
.b8 101
.b8 110
.b8 99
.b8 104
.b8 47
.b8 95
.b8 95
.b8 113
.b8 117
.b8 97
.b8 110
.b8 116
.b8 105
.b8 122
.b8 101
.b8 95
.b8 98
.b8 101
.b8 110
.b8 99
.b8 104
.b8 95
.b8 95
.b8 47
.b8 113
.b8 117
.b8 97
.b8 110
.b8 116
.b8 105
.b8 122
.b8 101
.b8 95
.b8 98
.b8 101
.b8 110
.b8 99
.b8 104
.b8 35
.b8 108
.b8 105
.b8 110
.b8 107
.b8 45
.b8 116
.b8 114
.b8 101
.b8 101
.b8 47
.b8 102
.b8 98
.b8 103
.b8 101
.b8 109
.b8 109
.b8 95
.b8 103
.b8 112
.b8 117
.b8 47
.b8 101
.b8 120
.b8 112
.b8 101
.b8 114
.b8 105
.b8 109
.b8 101
.b8 110
.b8 116
.b8 97
.b8 108
.b8 47
.b8 103
.b8 101
.b8 109
.b8 109
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 95
.b8 103
.b8 101
.b8 109
.b8 109
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 95
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 102
.b8 112
.b8 56
.b8 95
.b8 98
.b8 108
.b8 111
.b8 99
.b8 107
.b8 95
.b8 102
.b8 97
.b8 115
.b8 116
.b8 97
.b8 99
.b8 99
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 260
.b8 4
.b32 260
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 124
.b8 4
.b8 24
.b8 4
.b32 260
.b64 $L__tmp2
.b64 $L__tmp3
.b8 1
.b8 125
.b8 4
.b8 24
.b8 4
.b32 260
.b64 $L__tmp4
.b64 $L__tmp5
.b8 1
.b8 147
.b8 4
.b8 33
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
